{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"dashboards/","text":"Working with Dashboards, Charts and Metrics \u00b6 Introduction to the SignalFx Dashboards and charts Editing and creating charts Filtering and analytical functions Using formulas Introduction to SignalFlow 1. Introduction to the SignalFx UI \u00b6 Logon to the SignalFx organization you have been invited to. Hover over DASHBOARDS in the top menu, and then click on All Dashboards . A number of prebuilt dashboards are provided for you in your default view. If you are already receiving metrics through a Cloud API integration or the Smart Agent you will see relevant dashboards for these services in SignalFx. Among the dashboards you will see a Dashboard group called Sample Data . This group exists by default in all SignalFx accounts. Let's take a closer look at it. 2. Inspecting the Sample Data \u00b6 In this dashboard view expand the Sample Data dashboard group by clicking on it, and then click on the Intro to SignalFx dashboard. You will see a selection of sample charts. To learn more about charts you can click on the other sample dashboards ( PART 1 , PART 2 and PART 3 ). Let's take a look at the Sample charts. Click on the SAMPLE CHARTS dashboard name. In the Sample Charts dashboard you can see a selection of charts that show a sample of the various styles, colors and formats you can apply to your charts in the dashboards.","title":"Introduction"},{"location":"dashboards/#working-with-dashboards-charts-and-metrics","text":"Introduction to the SignalFx Dashboards and charts Editing and creating charts Filtering and analytical functions Using formulas Introduction to SignalFlow","title":"Working with Dashboards, Charts and Metrics"},{"location":"dashboards/#1-introduction-to-the-signalfx-ui","text":"Logon to the SignalFx organization you have been invited to. Hover over DASHBOARDS in the top menu, and then click on All Dashboards . A number of prebuilt dashboards are provided for you in your default view. If you are already receiving metrics through a Cloud API integration or the Smart Agent you will see relevant dashboards for these services in SignalFx. Among the dashboards you will see a Dashboard group called Sample Data . This group exists by default in all SignalFx accounts. Let's take a closer look at it.","title":"1. Introduction to the SignalFx UI"},{"location":"dashboards/#2-inspecting-the-sample-data","text":"In this dashboard view expand the Sample Data dashboard group by clicking on it, and then click on the Intro to SignalFx dashboard. You will see a selection of sample charts. To learn more about charts you can click on the other sample dashboards ( PART 1 , PART 2 and PART 3 ). Let's take a look at the Sample charts. Click on the SAMPLE CHARTS dashboard name. In the Sample Charts dashboard you can see a selection of charts that show a sample of the various styles, colors and formats you can apply to your charts in the dashboards.","title":"2. Inspecting the Sample Data"},{"location":"dashboards/adding-charts/","text":"Adding charts to dashboards \u00b6 Let's now save our chart. Click on Save as... and enter a name for your chart; use your initials like [YOUR INITIALS] Latency Chart and click OK . In the next window, find your email address in the list and select it, then click Ok . You will immediately be transported to the dashboard created under your selected group (make sure the group name on the top left is your email address). Last but not least, change the dashboard's name, by clicking the ... icon on the top right and selecting Rename . Enter a new name for your dashboard and click on Done . Congratulations! You have created your first chart and dashboard!","title":"Adding charts"},{"location":"dashboards/adding-charts/#adding-charts-to-dashboards","text":"Let's now save our chart. Click on Save as... and enter a name for your chart; use your initials like [YOUR INITIALS] Latency Chart and click OK . In the next window, find your email address in the list and select it, then click Ok . You will immediately be transported to the dashboard created under your selected group (make sure the group name on the top left is your email address). Last but not least, change the dashboard's name, by clicking the ... icon on the top right and selecting Rename . Enter a new name for your dashboard and click on Done . Congratulations! You have created your first chart and dashboard!","title":"Adding charts to dashboards"},{"location":"dashboards/datalinks/","text":"Integration with Splunk \u00b6 1. Introduction to Data links \u00b6 Data Links allow you to create dynamic links on properties that appear in a chart\u2019s data table and in list charts. Once enable this allows you to swiftly jump into external systems or SignalFx dashboards. 2. Configuring an integration with Splunk \u00b6 Informational exercise only This module is purely informational and serves only to educate how to link from SignalFx into Splunk Goto Settings \u2192 Organizations Settings \u2192 Global Data Links and click on New Link Here you will need to provide a Link Label e.g. Splunk. For the Link to use the dropdown and select Splunk . For Trigger there are 3 possible options: Any Metadata value - To display your link next to every property and every property value in a chart, select Any Metadata Value. Any Value of - To display your link next to a specified property type, select Any Value of. Property:Value Pair - To display your link next to a specified property value within a specified property type, select Property: Value Pair. To maximize the integration with Splunk, you are probably best selecting Any Metadata Value The URL value will be the your Splunk instance. The Minimum Time Window will be the time interval between values assigned to start_time and end_time variables and is equal to either this window or, when linking from a data table, the chart resolution if that is wider. For Property mapping this allows you define mappings between terms found in SignalFx and the external system. So, for example, SignalFx might have a property set called container_id but the external system uses container . 3. Using Data Links \u00b6 Once the Data Link is configured any Data Table, List Chart or from Kubernetes Navigator will provide an ellipsis ( ... ) allowing you to Examine in Splunk . Note Please note that because we configured Any Metadata value not all Data Links will yield results as there might not be corresponding logs for the property value.","title":"Integration to Splunk"},{"location":"dashboards/datalinks/#integration-with-splunk","text":"","title":"Integration with Splunk"},{"location":"dashboards/datalinks/#1-introduction-to-data-links","text":"Data Links allow you to create dynamic links on properties that appear in a chart\u2019s data table and in list charts. Once enable this allows you to swiftly jump into external systems or SignalFx dashboards.","title":"1. Introduction to Data links"},{"location":"dashboards/datalinks/#2-configuring-an-integration-with-splunk","text":"Informational exercise only This module is purely informational and serves only to educate how to link from SignalFx into Splunk Goto Settings \u2192 Organizations Settings \u2192 Global Data Links and click on New Link Here you will need to provide a Link Label e.g. Splunk. For the Link to use the dropdown and select Splunk . For Trigger there are 3 possible options: Any Metadata value - To display your link next to every property and every property value in a chart, select Any Metadata Value. Any Value of - To display your link next to a specified property type, select Any Value of. Property:Value Pair - To display your link next to a specified property value within a specified property type, select Property: Value Pair. To maximize the integration with Splunk, you are probably best selecting Any Metadata Value The URL value will be the your Splunk instance. The Minimum Time Window will be the time interval between values assigned to start_time and end_time variables and is equal to either this window or, when linking from a data table, the chart resolution if that is wider. For Property mapping this allows you define mappings between terms found in SignalFx and the external system. So, for example, SignalFx might have a property set called container_id but the external system uses container .","title":"2. Configuring an integration with Splunk"},{"location":"dashboards/datalinks/#3-using-data-links","text":"Once the Data Link is configured any Data Table, List Chart or from Kubernetes Navigator will provide an ellipsis ( ... ) allowing you to Examine in Splunk . Note Please note that because we configured Any Metadata value not all Data Links will yield results as there might not be corresponding logs for the property value.","title":"3. Using Data Links"},{"location":"dashboards/editing/","text":"Editing charts \u00b6 1. Edit Histogram Chart \u00b6 Click on the three dots ... on the Latency histogram chart in the Sample Data dashboard and then on Open (or you can click on the name of the chart which here is Latency histogram ). You will see the plot options, current plot and signal (metric) for the Latency histogram chart. Click on the different chart type icons to explore each of the visualizations. Notice their name while you click on or swipe over them. See how the chart changes. Note You can use different ways to visualize your metrics - you choose which chart type fits best for the visualization you want to have. For more info on the different chart types see Choosing a chart type . Click on the Line chart type and you will see the line plot. In the PLOT EDITOR tab under Signal you see the metric demo.trans.latency we are currently plotting. 2. Creating a new chart \u00b6 Let's now create a new chart and save it in a new dashboard! Click on the plus icon (top right of the UI) and from the drop down, click on Chart . You will now see a chart template like the following. Let's enter a metric to plot. We are going to use the metric demo.trans.latency . In the PLOT EDITOR tab under Signal enter demo.trans.latency . You will instantly see a number of Line plots, like below. The number 18 ts indicates that we are plotting 18 metric time series in the chart. Click on the DATA TABLE tab. You see now 18 rows, each representing a metics time series with a number of columns. If you swipe over the plot horizontally you will see the metrics in these columns at different times. In the demo_datacenter column you see that there are two data centers, Paris and Tokyo , for which we are getting metrics.","title":"Editing charts"},{"location":"dashboards/editing/#editing-charts","text":"","title":"Editing charts"},{"location":"dashboards/editing/#1-edit-histogram-chart","text":"Click on the three dots ... on the Latency histogram chart in the Sample Data dashboard and then on Open (or you can click on the name of the chart which here is Latency histogram ). You will see the plot options, current plot and signal (metric) for the Latency histogram chart. Click on the different chart type icons to explore each of the visualizations. Notice their name while you click on or swipe over them. See how the chart changes. Note You can use different ways to visualize your metrics - you choose which chart type fits best for the visualization you want to have. For more info on the different chart types see Choosing a chart type . Click on the Line chart type and you will see the line plot. In the PLOT EDITOR tab under Signal you see the metric demo.trans.latency we are currently plotting.","title":"1. Edit Histogram Chart"},{"location":"dashboards/editing/#2-creating-a-new-chart","text":"Let's now create a new chart and save it in a new dashboard! Click on the plus icon (top right of the UI) and from the drop down, click on Chart . You will now see a chart template like the following. Let's enter a metric to plot. We are going to use the metric demo.trans.latency . In the PLOT EDITOR tab under Signal enter demo.trans.latency . You will instantly see a number of Line plots, like below. The number 18 ts indicates that we are plotting 18 metric time series in the chart. Click on the DATA TABLE tab. You see now 18 rows, each representing a metics time series with a number of columns. If you swipe over the plot horizontally you will see the metrics in these columns at different times. In the demo_datacenter column you see that there are two data centers, Paris and Tokyo , for which we are getting metrics.","title":"2. Creating a new chart"},{"location":"dashboards/filtering/","text":"Using Filters \u00b6 1. Filtering and Analytics \u00b6 Let's now select the Paris datacenter to do some analytics - for that we will use a filter. Let's go back to the PLOT EDITOR tab and click on Add filter , wait until it automatically populates, choose demo_datacenter , and then Paris . In the F(x) column, add the analytic function Percentile:Aggregation , and leave the value to 95 (click outside to confirm). For info on the Percentile function and the other functions see Analytics reference . 2. Using Timeshift analytical function \u00b6 Let's now compare with older metrics. Click on ... and then on Clone in the dropdown to clone Signal A . You will see a new row identical to A , called B , both visible and plotted. For Signal B , in the F(x) column add the analytic function Timeshift and enter 7d (7 days = 1 week), and click outside to confirm. Click on the cog on the far right, and choose a Plot Color e.g. pink, to change color for the plot of B . Click on Close . Next, click into the field next to Time on the Override bar and choose Past Day from the dropdown. We now see plots for Signal A (the last day) as a blue plot, and 7 days ago in pink. In order to make this clearer we can click on the Area chart icon to change the visualization. We now have a better view of our two plots!","title":"Using filters"},{"location":"dashboards/filtering/#using-filters","text":"","title":"Using Filters"},{"location":"dashboards/filtering/#1-filtering-and-analytics","text":"Let's now select the Paris datacenter to do some analytics - for that we will use a filter. Let's go back to the PLOT EDITOR tab and click on Add filter , wait until it automatically populates, choose demo_datacenter , and then Paris . In the F(x) column, add the analytic function Percentile:Aggregation , and leave the value to 95 (click outside to confirm). For info on the Percentile function and the other functions see Analytics reference .","title":"1. Filtering and Analytics"},{"location":"dashboards/filtering/#2-using-timeshift-analytical-function","text":"Let's now compare with older metrics. Click on ... and then on Clone in the dropdown to clone Signal A . You will see a new row identical to A , called B , both visible and plotted. For Signal B , in the F(x) column add the analytic function Timeshift and enter 7d (7 days = 1 week), and click outside to confirm. Click on the cog on the far right, and choose a Plot Color e.g. pink, to change color for the plot of B . Click on Close . Next, click into the field next to Time on the Override bar and choose Past Day from the dropdown. We now see plots for Signal A (the last day) as a blue plot, and 7 days ago in pink. In order to make this clearer we can click on the Area chart icon to change the visualization. We now have a better view of our two plots!","title":"2. Using Timeshift analytical function"},{"location":"dashboards/formulas/","text":"Using Formulas \u00b6 1. Plotting differences \u00b6 Let's now plot the difference of all metric values for a day with 7 days in between. Click on Enter Formula then enter A-B (A minus B) and hide (deselect) all Signals using the eye, except C . We now see only the difference of all metric values of A and B being plotted. We see that we have some negative values on the plot because a metric value of B has some times larger value than the metric value of A at that time. Click on DATA TABLE and in that view swipe horizontally along the X axis to see the metric values at different times. 2. Using Absolute Value \u00b6 Click on PLOT EDITOR to get back to the Plot Editor view. Let's apply another function to get the values of C to positive values. Note By doing so we will see the difference between the metric values for a period of 24 hours with 7 days between. This difference can be used to see an alarming trend if we consider last week to be a baseline (the bigger the number - the more we deviate from the baseline) - but mainly we do this for you to get a bit more training on using analytical functions! In the PLOT EDITOR for C , under F(x) , click on Add Analytics and choose Absolute Value . You will see the C plot now having only positive values.","title":"Using formulas"},{"location":"dashboards/formulas/#using-formulas","text":"","title":"Using Formulas"},{"location":"dashboards/formulas/#1-plotting-differences","text":"Let's now plot the difference of all metric values for a day with 7 days in between. Click on Enter Formula then enter A-B (A minus B) and hide (deselect) all Signals using the eye, except C . We now see only the difference of all metric values of A and B being plotted. We see that we have some negative values on the plot because a metric value of B has some times larger value than the metric value of A at that time. Click on DATA TABLE and in that view swipe horizontally along the X axis to see the metric values at different times.","title":"1. Plotting differences"},{"location":"dashboards/formulas/#2-using-absolute-value","text":"Click on PLOT EDITOR to get back to the Plot Editor view. Let's apply another function to get the values of C to positive values. Note By doing so we will see the difference between the metric values for a period of 24 hours with 7 days between. This difference can be used to see an alarming trend if we consider last week to be a baseline (the bigger the number - the more we deviate from the baseline) - but mainly we do this for you to get a bit more training on using analytical functions! In the PLOT EDITOR for C , under F(x) , click on Add Analytics and choose Absolute Value . You will see the C plot now having only positive values.","title":"2. Using Absolute Value"},{"location":"dashboards/overlay/","text":"Using Overlays \u00b6 Let's overlay metrics and events to our initial plot to see if there is any correlation with high latency. To discover and add new metrics to the chart from the ones that are being sent to SignalFx already, click on Browse as highlighted in the screenshot below. In the METRICS sidebar on the right, enter demo and click on the search icon to search. Observe that the Find Metrics option is pre-selected. The metrics search is showing 3 metrics with demo in the name. Select demo.trans.count and click on the Add Plot button. Click on the blue eye icon next to C to hide that Signal, and on the greyed eye icon for Signal A to show it. On plot D , apply the Percentile:Aggregation function and set to 95 . Enter -1h in the Time frame for the entire chart. We see that there is a correlation between latency and number of transactions. Note Likewise we could check Find Events and find events like deployment events etc. to correlate with. Click on the the greater than sign icon to collapse the METRICS sidebar.","title":"Using overlays"},{"location":"dashboards/overlay/#using-overlays","text":"Let's overlay metrics and events to our initial plot to see if there is any correlation with high latency. To discover and add new metrics to the chart from the ones that are being sent to SignalFx already, click on Browse as highlighted in the screenshot below. In the METRICS sidebar on the right, enter demo and click on the search icon to search. Observe that the Find Metrics option is pre-selected. The metrics search is showing 3 metrics with demo in the name. Select demo.trans.count and click on the Add Plot button. Click on the blue eye icon next to C to hide that Signal, and on the greyed eye icon for Signal A to show it. On plot D , apply the Percentile:Aggregation function and set to 95 . Enter -1h in the Time frame for the entire chart. We see that there is a correlation between latency and number of transactions. Note Likewise we could check Find Events and find events like deployment events etc. to correlate with. Click on the the greater than sign icon to collapse the METRICS sidebar.","title":"Using Overlays"},{"location":"dashboards/signalflow/","text":"SignalFlow \u00b6 Let's take a look at SignalFlow - the analytics language of SignalFx that can be used to setup monitoring as code. Click on View SignalFlow . You will see the SignalFlow code that composes the chart we were working on. SignalFlow A = data ( 'demo.trans.latency' , filter = filter ( 'demo_datacenter' , 'Paris' )) . percentile ( pct = 95 ) . publish ( label = 'A' ) B = data ( 'demo.trans.latency' , filter = filter ( 'demo_datacenter' , 'Paris' )) . percentile ( pct = 95 ) . timeshift ( '1w' ) . publish ( label = 'B' , enable = False ) C = ( A - B ) . abs () . publish ( label = 'C' , enable = False ) D = data ( 'demo.trans.count' ) . percentile ( pct = 95 ) . publish ( label = 'D' ) SignalFlow is the analytics language of SignalFx. Among other benefits, it can be used to setup monitoring as code. For more info on SignalFlow see Getting started with SignalFlow . Click on View Builder to go back to the Chart Builder UI.","title":"SignalFlow"},{"location":"dashboards/signalflow/#signalflow","text":"Let's take a look at SignalFlow - the analytics language of SignalFx that can be used to setup monitoring as code. Click on View SignalFlow . You will see the SignalFlow code that composes the chart we were working on. SignalFlow A = data ( 'demo.trans.latency' , filter = filter ( 'demo_datacenter' , 'Paris' )) . percentile ( pct = 95 ) . publish ( label = 'A' ) B = data ( 'demo.trans.latency' , filter = filter ( 'demo_datacenter' , 'Paris' )) . percentile ( pct = 95 ) . timeshift ( '1w' ) . publish ( label = 'B' , enable = False ) C = ( A - B ) . abs () . publish ( label = 'C' , enable = False ) D = data ( 'demo.trans.count' ) . percentile ( pct = 95 ) . publish ( label = 'D' ) SignalFlow is the analytics language of SignalFx. Among other benefits, it can be used to setup monitoring as code. For more info on SignalFlow see Getting started with SignalFlow . Click on View Builder to go back to the Chart Builder UI.","title":"SignalFlow"},{"location":"detectors/detectors/","text":"Working with Detectors - Lab Summary \u00b6 Create a Detector from one of your charts Setting Alert conditions Running a pre-flight check Working with muting rules 1. Introduction \u00b6 A detector monitors a signal for conditions or issues that you care about. Those conditions or issues are expressed as one or more rules that trigger an alert when the conditions in the rules are met. Individual rules in a detector are labeled according to criticality: Info, Warning, Minor, Major, and Critical. For example, a detector that monitors the latency of an API call may go into a critical state when the latency is significantly higher than normal, as defined in the detector rules. 2. Create a detector from one of your charts \u00b6 In DASHBOARDS click on your dashboard group (the one with your email address) and then on the dashboard name where the chart you created in the previous lab resides or search for your previously created dashboard's name, and click on that dashboard's name in the results. We are now going to create a new detector from this chart. Once you see the chart, click on the bell icon on your chart and then on New Detector From Chart . In the text field next to Detector Name , ADD YOUR INITIALS before the proposed detector name. Naming the detector It's important that you add your initials in front of the proposed detector name. It should be something like this: LI's Latency Chart Detector . Click on Create Alert Rule In the Detector window, inside Alert signal , the Signal we will alert on is marked with a (blue) bell in the Alert on column. The bell indicates which Signal is being used to generate the alert. Click on Proceed to Alert Condition 3. Setting Alert condition \u00b6 In Alert condition , click on Static Threshold and then on Proceed to Alert Settings In Alert Settings , enter the value 290 in the Threshold field. In the same window change Time on top right to past day ( -1d ). 4. Alert pre-flight check \u00b6 SignalFx will now perform a pre-flight check after 5 seconds. See the Estimated alert count . Based on the current alert settings, the amount of alerts we would\u2019ve received in 1 day would have been approx. 18 . About pre-flight checks Once you set an alert condition, SignalFx shows how many alerts you would get based on the current settings, and in the timeframe set on the upper right corner - in this case, the past day. Immediately, the platform will start analyzing the signals with the current settings, and perform something we call a Pre-flight Check. This enables you to test the alert conditions using the historical data in the platform, to ensure the settings are logical and will not inadvertently generate an alert storm, removing the guess work from configuring alerts in a simple but very powerful way, only available using SignalFx. To read more about detector previewing, please visit this link Setting up detectors . Click on Proceed to Alert Message 5. Configuring the alert message \u00b6 In Alert message , under Severity choose Major . Click on Proceed to Alert Recipients Click on Add Recipient and then on your email address displayed as the first option. Notification Services That's the same as entering that email address OR you can enter another email address by clicking on E-mail... . That's just one example of the many Notification Services SignalFx has available. You can check this out by going to the INTEGRATIONS tab of the top menu, and see Notification Services . 6. Activating the alert \u00b6 Click on Proceed to Alert Activation In Activate... click on Activate Alert Rule If you want to get alerts quicker you can click back on Alert Settings and lower the value from 290 to say 280 . If you change the Time to -1h you can see how many alerts you might get with the threshold you have chosen based on the metics from the last 1 hour. Hover over ALERTS in the top menu and then click on Detectors . You will see you detector listed here. Congratulations ! You have created your first detector and activated it!","title":"Creating a Detector"},{"location":"detectors/detectors/#working-with-detectors-lab-summary","text":"Create a Detector from one of your charts Setting Alert conditions Running a pre-flight check Working with muting rules","title":"Working with Detectors - Lab Summary"},{"location":"detectors/detectors/#1-introduction","text":"A detector monitors a signal for conditions or issues that you care about. Those conditions or issues are expressed as one or more rules that trigger an alert when the conditions in the rules are met. Individual rules in a detector are labeled according to criticality: Info, Warning, Minor, Major, and Critical. For example, a detector that monitors the latency of an API call may go into a critical state when the latency is significantly higher than normal, as defined in the detector rules.","title":"1. Introduction"},{"location":"detectors/detectors/#2-create-a-detector-from-one-of-your-charts","text":"In DASHBOARDS click on your dashboard group (the one with your email address) and then on the dashboard name where the chart you created in the previous lab resides or search for your previously created dashboard's name, and click on that dashboard's name in the results. We are now going to create a new detector from this chart. Once you see the chart, click on the bell icon on your chart and then on New Detector From Chart . In the text field next to Detector Name , ADD YOUR INITIALS before the proposed detector name. Naming the detector It's important that you add your initials in front of the proposed detector name. It should be something like this: LI's Latency Chart Detector . Click on Create Alert Rule In the Detector window, inside Alert signal , the Signal we will alert on is marked with a (blue) bell in the Alert on column. The bell indicates which Signal is being used to generate the alert. Click on Proceed to Alert Condition","title":"2. Create a detector from one of your charts"},{"location":"detectors/detectors/#3-setting-alert-condition","text":"In Alert condition , click on Static Threshold and then on Proceed to Alert Settings In Alert Settings , enter the value 290 in the Threshold field. In the same window change Time on top right to past day ( -1d ).","title":"3. Setting Alert condition"},{"location":"detectors/detectors/#4-alert-pre-flight-check","text":"SignalFx will now perform a pre-flight check after 5 seconds. See the Estimated alert count . Based on the current alert settings, the amount of alerts we would\u2019ve received in 1 day would have been approx. 18 . About pre-flight checks Once you set an alert condition, SignalFx shows how many alerts you would get based on the current settings, and in the timeframe set on the upper right corner - in this case, the past day. Immediately, the platform will start analyzing the signals with the current settings, and perform something we call a Pre-flight Check. This enables you to test the alert conditions using the historical data in the platform, to ensure the settings are logical and will not inadvertently generate an alert storm, removing the guess work from configuring alerts in a simple but very powerful way, only available using SignalFx. To read more about detector previewing, please visit this link Setting up detectors . Click on Proceed to Alert Message","title":"4. Alert pre-flight check"},{"location":"detectors/detectors/#5-configuring-the-alert-message","text":"In Alert message , under Severity choose Major . Click on Proceed to Alert Recipients Click on Add Recipient and then on your email address displayed as the first option. Notification Services That's the same as entering that email address OR you can enter another email address by clicking on E-mail... . That's just one example of the many Notification Services SignalFx has available. You can check this out by going to the INTEGRATIONS tab of the top menu, and see Notification Services .","title":"5. Configuring the alert message"},{"location":"detectors/detectors/#6-activating-the-alert","text":"Click on Proceed to Alert Activation In Activate... click on Activate Alert Rule If you want to get alerts quicker you can click back on Alert Settings and lower the value from 290 to say 280 . If you change the Time to -1h you can see how many alerts you might get with the threshold you have chosen based on the metics from the last 1 hour. Hover over ALERTS in the top menu and then click on Detectors . You will see you detector listed here. Congratulations ! You have created your first detector and activated it!","title":"6. Activating the alert"},{"location":"detectors/muting/","text":"Working with Muting Rules - Lab Summary \u00b6 Learn how to configure how to mute Alerts 1. Learn how to configure muting your alerts \u00b6 There will be times when you might want to mute certain notifications. For example, if you want to schedule downtime for maintenance on a server or set of servers, or if you are testing new code or settings etc. For that you can use muting rules in SignalFx. Let's create one! Hover over ALERTS in the menu and from the drop down click on Detectors . You will see a list of active detectors. If you created an detector in Working with Detectors you can click on the three dots ... on the far right for that detector; if not, do that for another detector. From the drop-down click on Create Muting Rule... . In the Muting Rule window check Mute Indefinitely and enter a reason. Note This will mute the notifications permanently until you come back here and un-check this box or resume notifications for this detector. Click Next and in the new modal window confirm the muting rule setup. Click on Mute Indefinitely to confirm. You won't be receiving any email notifications from you detector until you resume notifications again. Let's now see how to do that! 2. Resuming notifications \u00b6 To Resume notifications, hover over ALERTS in the top menu and click on Muting Rules . You will see the name of the detector you muted notifications for under Detector . Click on the thee dots ... on the far right. Click on Resume Notifications . Click on Resume to confirm and resume notifications for this detector. Congratulations! You have now resumed your alert notifications!","title":"Creating a Muting Rule"},{"location":"detectors/muting/#working-with-muting-rules-lab-summary","text":"Learn how to configure how to mute Alerts","title":"Working with Muting Rules - Lab Summary"},{"location":"detectors/muting/#1-learn-how-to-configure-muting-your-alerts","text":"There will be times when you might want to mute certain notifications. For example, if you want to schedule downtime for maintenance on a server or set of servers, or if you are testing new code or settings etc. For that you can use muting rules in SignalFx. Let's create one! Hover over ALERTS in the menu and from the drop down click on Detectors . You will see a list of active detectors. If you created an detector in Working with Detectors you can click on the three dots ... on the far right for that detector; if not, do that for another detector. From the drop-down click on Create Muting Rule... . In the Muting Rule window check Mute Indefinitely and enter a reason. Note This will mute the notifications permanently until you come back here and un-check this box or resume notifications for this detector. Click Next and in the new modal window confirm the muting rule setup. Click on Mute Indefinitely to confirm. You won't be receiving any email notifications from you detector until you resume notifications again. Let's now see how to do that!","title":"1. Learn how to configure muting your alerts"},{"location":"detectors/muting/#2-resuming-notifications","text":"To Resume notifications, hover over ALERTS in the top menu and click on Muting Rules . You will see the name of the detector you muted notifications for under Detector . Click on the thee dots ... on the far right. Click on Resume Notifications . Click on Resume to confirm and resume notifications for this detector. Congratulations! You have now resumed your alert notifications!","title":"2. Resuming notifications"},{"location":"eks-cluster/readme-b/","text":"Steps to provision EKS Cluster in a Jump Box \u00b6 Jumpbox configuration \u00b6 Before running any commands start GNU screen session \u00b6 screen Useful screen command \u00b6 screen -ls # lists sessions screen -x <session number> # connects to your previous session Configure account \u00b6 export ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.region') Test the variables \u00b6 test -n \"$AWS_REGION\" && echo AWS_REGION is \"$AWS_REGION\" || echo AWS_REGION is not set Configure your acccount on the jumpbox \u00b6 echo \"export ACCOUNT_ID=${ACCOUNT_ID}\" | tee -a ~/.bash_profile echo \"export AWS_REGION=${AWS_REGION}\" | tee -a ~/.bash_profile aws configure set default.region ${AWS_REGION} aws configure get default.region Generate and Import Key Pair \u00b6 ssh-keygen -t rsa -N \"\" -f ~/.ssh/id_rsa aws ec2 import-key-pair --key-name \"eksworkshop\" --public-key-material file://~/.ssh/id_rsa.pub aws kms create-alias --alias-name alias/eksworkshop --target-key-id $(aws kms create-key --query KeyMetadata.Arn --output text) export MASTER_ARN=$(aws kms describe-key --key-id alias/eksworkshop --query KeyMetadata.Arn --output text) echo \"export MASTER_ARN=${MASTER_ARN}\" | tee -a ~/.bash_profile Enable bash completion \u00b6 eksctl completion bash >> ~/.bash_completion . /etc/profile.d/bash_completion.sh . ~/.bash_completion Create Cluster yaml \u00b6 cat << EOF > eksworkshop.yaml --- apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eksworkshop-eksctl region: ${AWS_REGION} managedNodeGroups: - name: nodegroup desiredCapacity: iam: withAddonPolicies: albIngress: true secretsEncryption: keyARN: ${MASTER_ARN} EOF Launch the cluster. This will take ~15 min. \u00b6 eksctl create cluster -f eksworkshop.yaml Update kubectl configuration \u00b6 aws eks --region us-east-1 update-kubeconfig --name eksworkshop-eksctl Test kubectl \u00b6 kubectl get nodes # if we see our 2 nodes, we know we have authenticated correctly","title":"Create EKS Cluster"},{"location":"eks-cluster/readme-b/#steps-to-provision-eks-cluster-in-a-jump-box","text":"","title":"Steps to provision EKS Cluster in a Jump Box"},{"location":"eks-cluster/readme-b/#jumpbox-configuration","text":"","title":"Jumpbox configuration"},{"location":"eks-cluster/readme-b/#before-running-any-commands-start-gnu-screen-session","text":"screen","title":"Before running any commands start GNU screen session"},{"location":"eks-cluster/readme-b/#useful-screen-command","text":"screen -ls # lists sessions screen -x <session number> # connects to your previous session","title":"Useful screen command"},{"location":"eks-cluster/readme-b/#configure-account","text":"export ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.region')","title":"Configure account"},{"location":"eks-cluster/readme-b/#test-the-variables","text":"test -n \"$AWS_REGION\" && echo AWS_REGION is \"$AWS_REGION\" || echo AWS_REGION is not set","title":"Test the variables"},{"location":"eks-cluster/readme-b/#configure-your-acccount-on-the-jumpbox","text":"echo \"export ACCOUNT_ID=${ACCOUNT_ID}\" | tee -a ~/.bash_profile echo \"export AWS_REGION=${AWS_REGION}\" | tee -a ~/.bash_profile aws configure set default.region ${AWS_REGION} aws configure get default.region","title":"Configure your acccount on the jumpbox"},{"location":"eks-cluster/readme-b/#generate-and-import-key-pair","text":"ssh-keygen -t rsa -N \"\" -f ~/.ssh/id_rsa aws ec2 import-key-pair --key-name \"eksworkshop\" --public-key-material file://~/.ssh/id_rsa.pub aws kms create-alias --alias-name alias/eksworkshop --target-key-id $(aws kms create-key --query KeyMetadata.Arn --output text) export MASTER_ARN=$(aws kms describe-key --key-id alias/eksworkshop --query KeyMetadata.Arn --output text) echo \"export MASTER_ARN=${MASTER_ARN}\" | tee -a ~/.bash_profile","title":"Generate and Import Key Pair"},{"location":"eks-cluster/readme-b/#enable-bash-completion","text":"eksctl completion bash >> ~/.bash_completion . /etc/profile.d/bash_completion.sh . ~/.bash_completion","title":"Enable bash completion"},{"location":"eks-cluster/readme-b/#create-cluster-yaml","text":"cat << EOF > eksworkshop.yaml --- apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eksworkshop-eksctl region: ${AWS_REGION} managedNodeGroups: - name: nodegroup desiredCapacity: iam: withAddonPolicies: albIngress: true secretsEncryption: keyARN: ${MASTER_ARN} EOF","title":"Create Cluster yaml"},{"location":"eks-cluster/readme-b/#launch-the-cluster-this-will-take-15-min","text":"eksctl create cluster -f eksworkshop.yaml","title":"Launch the cluster.  This will take ~15 min."},{"location":"eks-cluster/readme-b/#update-kubectl-configuration","text":"aws eks --region us-east-1 update-kubeconfig --name eksworkshop-eksctl","title":"Update kubectl configuration"},{"location":"eks-cluster/readme-b/#test-kubectl","text":"kubectl get nodes # if we see our 2 nodes, we know we have authenticated correctly","title":"Test kubectl"},{"location":"module-support/cleanup/","text":"Post Workshop Clean Up \u00b6 Multipass Once you have finished with this Workshop exit from the Multipass instance to get back to your system command prompt. Enter the following to delete and purge the Multipass instance: multipass delete --purge ${ INSTANCE } AWS/EC2 Once you have finished with this Workshop exit from the AWS/EC2 instance to get back to your system command prompt. We will use Terraform to destroy the instance with the parameters you used in Smart Agent module: cd ~/workshop/ec2 terraform destroy -var = \"aws_instance_count=1\" -var = \"instance_type=1\" Enter Instance Count , Provide the desired region and Select instance type required . When prompted type yes to confirm you want to destroy, this will take a while to complete. aws_instance.observability-instance[0]: Destroying... [id=i-088560a5f6e2bbdbb] aws_instance.observability-instance[0]: Still destroying... [id=i-088560a5f6e2bbdbb, 10s elapsed] aws_instance.observability-instance[0]: Still destroying... [id=i-088560a5f6e2bbdbb, 20s elapsed] aws_instance.observability-instance[0]: Still destroying... [id=i-088560a5f6e2bbdbb, 30s elapsed] aws_instance.observability-instance[0]: Still destroying... [id=i-088560a5f6e2bbdbb, 40s elapsed] aws_instance.observability-instance[0]: Still destroying... [id=i-088560a5f6e2bbdbb, 50s elapsed] aws_instance.observability-instance[0]: Destruction complete after 56s aws_security_group.instance: Destroying... [id=sg-0d6841fbeef022a9f] aws_security_group.instance: Destruction complete after 2s","title":"Post Workshop Clean Up"},{"location":"module-support/cleanup/#post-workshop-clean-up","text":"Multipass Once you have finished with this Workshop exit from the Multipass instance to get back to your system command prompt. Enter the following to delete and purge the Multipass instance: multipass delete --purge ${ INSTANCE } AWS/EC2 Once you have finished with this Workshop exit from the AWS/EC2 instance to get back to your system command prompt. We will use Terraform to destroy the instance with the parameters you used in Smart Agent module: cd ~/workshop/ec2 terraform destroy -var = \"aws_instance_count=1\" -var = \"instance_type=1\" Enter Instance Count , Provide the desired region and Select instance type required . When prompted type yes to confirm you want to destroy, this will take a while to complete. aws_instance.observability-instance[0]: Destroying... [id=i-088560a5f6e2bbdbb] aws_instance.observability-instance[0]: Still destroying... [id=i-088560a5f6e2bbdbb, 10s elapsed] aws_instance.observability-instance[0]: Still destroying... [id=i-088560a5f6e2bbdbb, 20s elapsed] aws_instance.observability-instance[0]: Still destroying... [id=i-088560a5f6e2bbdbb, 30s elapsed] aws_instance.observability-instance[0]: Still destroying... [id=i-088560a5f6e2bbdbb, 40s elapsed] aws_instance.observability-instance[0]: Still destroying... [id=i-088560a5f6e2bbdbb, 50s elapsed] aws_instance.observability-instance[0]: Destruction complete after 56s aws_security_group.instance: Destroying... [id=sg-0d6841fbeef022a9f] aws_security_group.instance: Destruction complete after 2s","title":"Post Workshop Clean Up"},{"location":"module-support/hotrod-eks/","text":"Deploying Hot R.O.D. in AWS/EKS \u00b6 Enabling \u00b5APM An Organization needs to be pre-provisioned as a \u00b5APM entitlement is required for the purposes of this module. Please contact someone from SignalFx to get a trial instance with \u00b5APM enabled if you don\u2019t have one already. To check if you have an Organization with \u00b5APM enabled, just login to SignalFx and check that you have the \u00b5APM tab on the top navbar next to Dashboards. 1. Launch the Multipass instance \u00b6 If you have not already completed the Smart Agent Preparation , then please do so, otherwise jump to Step #2 2. Create environment variables \u00b6 Create the following environment variables for SignalFx and AWS to use in the proceeding steps: SignalFx export ACCESS_TOKEN=[ACCESS_TOKEN] export REALM=[REALM e.g. us1] AWS export AWS_ACCESS_KEY_ID=[AWS Access Key] export AWS_SECRET_ACCESS_KEY=[AWS Secret Access Key] export AWS_DEFAULT_REGION=[e.g. us-east-1] export AWS_DEFAULT_OUTPUT=json export EKS_CLUSTER_NAME=$(hostname)-APP-DEV You can check for the latest SignalFx Smart Agent release on Github . 3. Configure AWS CLI for your account \u00b6 Use the AWS CLI to configure access to your AWS environment. The environment variables configured above mean you can just hit enter on each of the prompts to accept the values: Shell Command aws configure Output AWS Access Key ID [****************TVAQ]: AWS Secret Access Key [****************MkB4]: Default region name [us-east-1]: Default output format [json]: 4. Create a cluster running Amazon Elastic Kubernetes Service (EKS) \u00b6 Shell Command eksctl create cluster \\ --name $EKS_CLUSTER_NAME \\ --region $AWS_DEFAULT_REGION \\ --node-type t3.medium \\ --nodes-min 3 \\ --nodes-max 7 \\ --version=1.15 Output [\u2139] eksctl version 0.16.0 [\u2139] using region us-east-1 [\u2139] setting availability zones to [us-east-1a us-east-1f] [\u2139] subnets for us-east-1a - public:192.168.0.0/19 private:192.168.64.0/19 [\u2139] subnets for us-east-1f - public:192.168.32.0/19 private:192.168.96.0/19 [\u2139] nodegroup \"ng-371a784a\" will use \"ami-0e5bb2367e692b807\" [AmazonLinux2/1.15] [\u2139] using Kubernetes version 1.15 [\u2139] creating EKS cluster \"EKS-APP-DEV\" in \"us-east-1\" region with un-managed nodes [\u2139] will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup [\u2139] if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-1 --cluster=EKS-APP-DEV' [\u2139] CloudWatch logging will not be enabled for cluster \"EKS-APP-DEV\" in \"us-east-1\" [\u2139] you can enable it with 'eksctl utils update-cluster-logging --region=us-east-1 --cluster=EKS-APP-DEV' [\u2139] Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \"EKS-APP-DEV\" in \"us-east-1\" [\u2139] 2 sequential tasks: { create cluster control plane \"EKS-APP-DEV\", create nodegroup \"ng-371a784a\" } [\u2139] building cluster stack \"eksctl-EKS-APP-DEV-cluster\" [\u2139] deploying stack \"eksctl-EKS-APP-DEV-cluster\" [\u2139] building nodegroup stack \"eksctl-EKS-APP-DEV-nodegroup-ng-371a784a\" [\u2139] deploying stack \"eksctl-EKS-APP-DEV-nodegroup-ng-371a784a\" [\u2714] all EKS cluster resources for \"EKS-APP-DEV\" have been created [!] unable to write kubeconfig , please retry with 'eksctl utils write-kubeconfig -n EKS-APP-DEV': unable to modify kubeconfig /home/ubuntu/.kube/config: open /etc/rancher/k3s/k3s.yaml.lock: permission denied [\u2139] adding identity \"arn:aws:iam::327192335161:role/eksctl-EKS-APP-DEV-nodegroup-ng-3-NodeInstanceRole-2RMH7RBODD62\" to auth ConfigMap [\u2139] nodegroup \"ng-371a784a\" has 0 node(s) [\u2139] waiting for at least 3 node(s) to become ready in \"ng-371a784a\" [\u2139] nodegroup \"ng-371a784a\" has 3 node(s) [\u2139] node \"ip-192-168-35-104.ec2.internal\" is ready [\u2139] node \"ip-192-168-52-88.ec2.internal\" is ready [\u2139] node \"ip-192-168-8-236.ec2.internal\" is ready [\u2714] EKS cluster \"EKS-APP-DEV\" in \"us-east-1\" region is ready This may take some time (10-15 minutes). Ensure you see your cluster active in AWS EKS console before proceeding. Note You can ignore the error about unable to write kubeconfig as we address this below. Once complete update your kubeconfig to allow kubectl access to the cluster: Shell Command sudo eksctl utils write-kubeconfig -n $EKS_CLUSTER_NAME 5. Deploy SignalFx SmartAgent to your EKS Cluster \u00b6 Add the SignalFx Helm chart repository to Helm: Shell Command helm repo add signalfx https://dl.signalfx.com/helm-repo Output \"signalfx\" has been added to your repositories Ensure the latest state of the SignalFx Helm repository: Shell Command helm repo update Output Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"signalfx\" chart repository Install the Smart Agent Helm chart with the following commands: Shell Command helm install \\ --set signalFxAccessToken=$ACCESS_TOKEN \\ --set clusterName=$EKS_CLUSTER_NAME \\ --set signalFxRealm=$REALM \\ --set kubeletAPI.url=https://localhost:10250 \\ --set traceEndpointUrl=https://ingest.$REALM.signalfx.com/v2/trace \\ signalfx-agent signalfx/signalfx-agent \\ -f workshop/k3s/values.yaml Output NAME: signalfx-agent LAST DEPLOYED: Mon Apr 13 14:23:19 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The SignalFx agent is being deployed in your Kubernetes cluster. You should see metrics flowing once the agent image is downloaded and started (this may take a few minutes since it has to download the agent container image). Assuming you are logged into SignalFx in your browser, visit https://app.us0.signalfx.com/#/navigator/kubernetes%20pods/kubernetes%20pods to see all of the pods in your cluster. Validate cluster looks healthy in SignalFx Kubernetes Navigator dashboard 6. Deploy Hot R.O.D. Application to EKS \u00b6 Shell Command kubectl apply -f ~/workshop/apm/hotrod/k8s/deployment.yaml To ensure the Hot R.O.D. application is running see examples below: Shell Command kubectl get pods Output NAME READY STATUS RESTARTS AGE hotrod-7564774bf5-vjpfw 1/1 Running 0 47h signalfx-agent-jmq4f 1/1 Running 0 138m signalfx-agent-nk8p9 1/1 Running 0 138m signalfx-agent-q5tzh 1/1 Running 0 138m You then need find the IP address assigned to the Hot R.O.D. service: Shell Command kubectl get svc Output NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hotrod LoadBalancer 10.100.188.249 af26ce80ef2e14c9292ae5b4bc0d2dd0-1826890352.us-east-2.elb.amazonaws.com 8080:32521/TCP 47h kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 3d1h Create an environment variable for the IP address and port that the Hot R.O.D. application is exposed on: Shell Command HOTROD_ENDPOINT=$(kubectl get svc hotrod -n default -o jsonpath='{.spec.clusterIP}:{.spec.ports[0].port}') You can view / exercise Hot R.O.D. yourself in a browser by opening the EXTERNAL-IP:PORT as shown above e.g. Example URL https://af26ce80ef2e14c9292ae5b4bc0d2dd0-1826890352.us-east-2.elb.amazonaws.com:8080 7. Generate some traffic to the application using Siege Benchmark \u00b6 Shell Command siege -r10 -c10 \"http:// $HOTROD_ENDPOINT /dispatch?customer=392&nonse=0.17041229755366172\" Create some errors with an invalid customer number Shell Command siege -r10 -c10 \"http:// $HOTROD_ENDPOINT /dispatch?customer=391&nonse=0.17041229755366172\" You should now be able to exercise SignalFx APM dashboards. 8. Cleaning up \u00b6 To delete entire EKS cluster: Shell Command eksctl delete cluster $EKS_CLUSTER_NAME Output [\u2139] eksctl version 0.16.0 [\u2139] using region us-east-1 [\u2139] deleting EKS cluster \"RWC-APP-DEV\" [\u2139] deleted 0 Fargate profile(s) [\u2139] cleaning up LoadBalancer services [\u2139] 2 sequential tasks: { delete nodegroup \"ng-371a784a\", delete cluster control plane \"EKS-APP-DEV\" [async] } [\u2139] will delete stack \"eksctl-EKS-APP-DEV-nodegroup-ng-371a784a\" [\u2139] waiting for stack \"eksctl-EKS-APP-DEV-nodegroup-ng-371a784a\" to get deleted [\u2139] will delete stack \"eksctl-EKS-APP-DEV-cluster\" [\u2714] all cluster resources were deleted Or to delete individual components: Shell Command kubectl delete deploy/hotrod svc/hotrod helm delete signalfx-agent To switch back to using the local K3s cluster: Shell Command sudo kubectl config use-context default","title":"Deploying Hot R.O.D. in AWS/EKS"},{"location":"module-support/hotrod-eks/#deploying-hot-rod-in-awseks","text":"Enabling \u00b5APM An Organization needs to be pre-provisioned as a \u00b5APM entitlement is required for the purposes of this module. Please contact someone from SignalFx to get a trial instance with \u00b5APM enabled if you don\u2019t have one already. To check if you have an Organization with \u00b5APM enabled, just login to SignalFx and check that you have the \u00b5APM tab on the top navbar next to Dashboards.","title":"Deploying Hot R.O.D. in AWS/EKS"},{"location":"module-support/hotrod-eks/#1-launch-the-multipass-instance","text":"If you have not already completed the Smart Agent Preparation , then please do so, otherwise jump to Step #2","title":"1. Launch the Multipass instance"},{"location":"module-support/hotrod-eks/#2-create-environment-variables","text":"Create the following environment variables for SignalFx and AWS to use in the proceeding steps: SignalFx export ACCESS_TOKEN=[ACCESS_TOKEN] export REALM=[REALM e.g. us1] AWS export AWS_ACCESS_KEY_ID=[AWS Access Key] export AWS_SECRET_ACCESS_KEY=[AWS Secret Access Key] export AWS_DEFAULT_REGION=[e.g. us-east-1] export AWS_DEFAULT_OUTPUT=json export EKS_CLUSTER_NAME=$(hostname)-APP-DEV You can check for the latest SignalFx Smart Agent release on Github .","title":"2. Create environment variables"},{"location":"module-support/hotrod-eks/#3-configure-aws-cli-for-your-account","text":"Use the AWS CLI to configure access to your AWS environment. The environment variables configured above mean you can just hit enter on each of the prompts to accept the values: Shell Command aws configure Output AWS Access Key ID [****************TVAQ]: AWS Secret Access Key [****************MkB4]: Default region name [us-east-1]: Default output format [json]:","title":"3. Configure AWS CLI for your account"},{"location":"module-support/hotrod-eks/#4-create-a-cluster-running-amazon-elastic-kubernetes-service-eks","text":"Shell Command eksctl create cluster \\ --name $EKS_CLUSTER_NAME \\ --region $AWS_DEFAULT_REGION \\ --node-type t3.medium \\ --nodes-min 3 \\ --nodes-max 7 \\ --version=1.15 Output [\u2139] eksctl version 0.16.0 [\u2139] using region us-east-1 [\u2139] setting availability zones to [us-east-1a us-east-1f] [\u2139] subnets for us-east-1a - public:192.168.0.0/19 private:192.168.64.0/19 [\u2139] subnets for us-east-1f - public:192.168.32.0/19 private:192.168.96.0/19 [\u2139] nodegroup \"ng-371a784a\" will use \"ami-0e5bb2367e692b807\" [AmazonLinux2/1.15] [\u2139] using Kubernetes version 1.15 [\u2139] creating EKS cluster \"EKS-APP-DEV\" in \"us-east-1\" region with un-managed nodes [\u2139] will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup [\u2139] if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-1 --cluster=EKS-APP-DEV' [\u2139] CloudWatch logging will not be enabled for cluster \"EKS-APP-DEV\" in \"us-east-1\" [\u2139] you can enable it with 'eksctl utils update-cluster-logging --region=us-east-1 --cluster=EKS-APP-DEV' [\u2139] Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \"EKS-APP-DEV\" in \"us-east-1\" [\u2139] 2 sequential tasks: { create cluster control plane \"EKS-APP-DEV\", create nodegroup \"ng-371a784a\" } [\u2139] building cluster stack \"eksctl-EKS-APP-DEV-cluster\" [\u2139] deploying stack \"eksctl-EKS-APP-DEV-cluster\" [\u2139] building nodegroup stack \"eksctl-EKS-APP-DEV-nodegroup-ng-371a784a\" [\u2139] deploying stack \"eksctl-EKS-APP-DEV-nodegroup-ng-371a784a\" [\u2714] all EKS cluster resources for \"EKS-APP-DEV\" have been created [!] unable to write kubeconfig , please retry with 'eksctl utils write-kubeconfig -n EKS-APP-DEV': unable to modify kubeconfig /home/ubuntu/.kube/config: open /etc/rancher/k3s/k3s.yaml.lock: permission denied [\u2139] adding identity \"arn:aws:iam::327192335161:role/eksctl-EKS-APP-DEV-nodegroup-ng-3-NodeInstanceRole-2RMH7RBODD62\" to auth ConfigMap [\u2139] nodegroup \"ng-371a784a\" has 0 node(s) [\u2139] waiting for at least 3 node(s) to become ready in \"ng-371a784a\" [\u2139] nodegroup \"ng-371a784a\" has 3 node(s) [\u2139] node \"ip-192-168-35-104.ec2.internal\" is ready [\u2139] node \"ip-192-168-52-88.ec2.internal\" is ready [\u2139] node \"ip-192-168-8-236.ec2.internal\" is ready [\u2714] EKS cluster \"EKS-APP-DEV\" in \"us-east-1\" region is ready This may take some time (10-15 minutes). Ensure you see your cluster active in AWS EKS console before proceeding. Note You can ignore the error about unable to write kubeconfig as we address this below. Once complete update your kubeconfig to allow kubectl access to the cluster: Shell Command sudo eksctl utils write-kubeconfig -n $EKS_CLUSTER_NAME","title":"4. Create a cluster running Amazon Elastic Kubernetes Service (EKS)"},{"location":"module-support/hotrod-eks/#5-deploy-signalfx-smartagent-to-your-eks-cluster","text":"Add the SignalFx Helm chart repository to Helm: Shell Command helm repo add signalfx https://dl.signalfx.com/helm-repo Output \"signalfx\" has been added to your repositories Ensure the latest state of the SignalFx Helm repository: Shell Command helm repo update Output Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"signalfx\" chart repository Install the Smart Agent Helm chart with the following commands: Shell Command helm install \\ --set signalFxAccessToken=$ACCESS_TOKEN \\ --set clusterName=$EKS_CLUSTER_NAME \\ --set signalFxRealm=$REALM \\ --set kubeletAPI.url=https://localhost:10250 \\ --set traceEndpointUrl=https://ingest.$REALM.signalfx.com/v2/trace \\ signalfx-agent signalfx/signalfx-agent \\ -f workshop/k3s/values.yaml Output NAME: signalfx-agent LAST DEPLOYED: Mon Apr 13 14:23:19 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The SignalFx agent is being deployed in your Kubernetes cluster. You should see metrics flowing once the agent image is downloaded and started (this may take a few minutes since it has to download the agent container image). Assuming you are logged into SignalFx in your browser, visit https://app.us0.signalfx.com/#/navigator/kubernetes%20pods/kubernetes%20pods to see all of the pods in your cluster. Validate cluster looks healthy in SignalFx Kubernetes Navigator dashboard","title":"5. Deploy SignalFx SmartAgent to your EKS Cluster"},{"location":"module-support/hotrod-eks/#6-deploy-hot-rod-application-to-eks","text":"Shell Command kubectl apply -f ~/workshop/apm/hotrod/k8s/deployment.yaml To ensure the Hot R.O.D. application is running see examples below: Shell Command kubectl get pods Output NAME READY STATUS RESTARTS AGE hotrod-7564774bf5-vjpfw 1/1 Running 0 47h signalfx-agent-jmq4f 1/1 Running 0 138m signalfx-agent-nk8p9 1/1 Running 0 138m signalfx-agent-q5tzh 1/1 Running 0 138m You then need find the IP address assigned to the Hot R.O.D. service: Shell Command kubectl get svc Output NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hotrod LoadBalancer 10.100.188.249 af26ce80ef2e14c9292ae5b4bc0d2dd0-1826890352.us-east-2.elb.amazonaws.com 8080:32521/TCP 47h kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 3d1h Create an environment variable for the IP address and port that the Hot R.O.D. application is exposed on: Shell Command HOTROD_ENDPOINT=$(kubectl get svc hotrod -n default -o jsonpath='{.spec.clusterIP}:{.spec.ports[0].port}') You can view / exercise Hot R.O.D. yourself in a browser by opening the EXTERNAL-IP:PORT as shown above e.g. Example URL https://af26ce80ef2e14c9292ae5b4bc0d2dd0-1826890352.us-east-2.elb.amazonaws.com:8080","title":"6. Deploy Hot R.O.D. Application to EKS"},{"location":"module-support/hotrod-eks/#7-generate-some-traffic-to-the-application-using-siege-benchmark","text":"Shell Command siege -r10 -c10 \"http:// $HOTROD_ENDPOINT /dispatch?customer=392&nonse=0.17041229755366172\" Create some errors with an invalid customer number Shell Command siege -r10 -c10 \"http:// $HOTROD_ENDPOINT /dispatch?customer=391&nonse=0.17041229755366172\" You should now be able to exercise SignalFx APM dashboards.","title":"7. Generate some traffic to the application using Siege Benchmark"},{"location":"module-support/hotrod-eks/#8-cleaning-up","text":"To delete entire EKS cluster: Shell Command eksctl delete cluster $EKS_CLUSTER_NAME Output [\u2139] eksctl version 0.16.0 [\u2139] using region us-east-1 [\u2139] deleting EKS cluster \"RWC-APP-DEV\" [\u2139] deleted 0 Fargate profile(s) [\u2139] cleaning up LoadBalancer services [\u2139] 2 sequential tasks: { delete nodegroup \"ng-371a784a\", delete cluster control plane \"EKS-APP-DEV\" [async] } [\u2139] will delete stack \"eksctl-EKS-APP-DEV-nodegroup-ng-371a784a\" [\u2139] waiting for stack \"eksctl-EKS-APP-DEV-nodegroup-ng-371a784a\" to get deleted [\u2139] will delete stack \"eksctl-EKS-APP-DEV-cluster\" [\u2714] all cluster resources were deleted Or to delete individual components: Shell Command kubectl delete deploy/hotrod svc/hotrod helm delete signalfx-agent To switch back to using the local K3s cluster: Shell Command sudo kubectl config use-context default","title":"8. Cleaning up"},{"location":"module-support/vm/","text":"Lab Summary \u00b6 Deploy SignalFx Smart Agent via install script on a VM Confirm the Smart Agent is working and sending data Use multipass to create a vanilla Ubuntu VM and shell into it. You can also use a Linux-based VM with your cloud provider of choice. Replace [INITIALS] with your actual initials. Shell Command multipass launch [ INITIALS ] -vm multipass shell [ INITIALs ] -vm 1. Deploy SignalFx Smart Agent via install script on a VM \u00b6 You will need to obtain your Access Token from the SignalFx UI. You can find your Access Token by clicking on your profile icon on the top right of the SignalFx UI. Then select Organization Settings \u2192 Access Tokens . Expand the Default token, then click on Show Token to expose your token. Later in the lab you can come back here and click the Copy button which will copy it to your clipboard so you can paste it when you need to provide an access token in the lab. You will also need to obtain the name of the Realm for your SignalFx account. Click on the profile icon again, but this time select 'My Profile'. The Ream can be found in the middle of the page within the Organizations section. In this example it is us1 . SignalFx maintains a shell script to install on supported distributions. Copy the script below and replace $REALM and $ACCESS_TOKEN with the values found in previous screen: Shell Command curl -sSL https://dl.signalfx.com/signalfx-agent.sh > /tmp/signalfx-agent.sh sudo sh /tmp/signalfx-agent.sh --realm $REALM -- $ACCESS_TOKEN Once the installation is complete check the status of the agent. Shell Command signalfx-agent status Output SignalFx Agent version: 5.1.2 Agent uptime: 4s Observers active: host Active Monitors: 9 Configured Monitors: 9 Discovered Endpoint Count: 16 Bad Monitor Config: None Global Dimensions: {host: as-k3s} GlobalSpanTags: map[] Datapoints sent (last minute): 0 Datapoints failed (last minute): 0 Datapoints overwritten (total): 0 Events Sent (last minute): 0 Trace Spans Sent (last minute): 0 Trace Spans overwritten (total): 0 Additional status commands: signalfx-agent status config - show resolved config in use by agent signalfx-agent status endpoints - show discovered endpoints signalfx-agent status monitors - show active monitors signalfx-agent status all - show everything Important Make a note of the value displayed for host in the Global Dimensions section of the output, as you need this later! 2. Confirm the Smart Agent is working and sending data \u00b6 To see the Metrics that the Smart Agent is sending to SignalFx, please goto the SignalFX UI, and select Infrastructure \u2192 Hosts to see the lists of hosts. Here you see a list of the Nodes that have an Smart Agent installed and are reporting into SignalFx. Make sure you see your Multipass or AWS/EC2 instance in the list of hosts. (The hostname from the previous section) You can also set a filter for just your instance by selecting the host: attribute, followed by picking the name of your host from the drop down list. Click on the link to your host from the list, this wil take you to the overview page of your host. Make sure you have the SYSTEM METRIC tab selected. Here you can see various charts that relate to the health of your host, like CPU & Memory Used%, Disk I/O and many more. You can also see the list of services running on your host by selecting the PROCESSES tab. Take a moment to explore the various charts and the Processes list.","title":"Lab Summary"},{"location":"module-support/vm/#lab-summary","text":"Deploy SignalFx Smart Agent via install script on a VM Confirm the Smart Agent is working and sending data Use multipass to create a vanilla Ubuntu VM and shell into it. You can also use a Linux-based VM with your cloud provider of choice. Replace [INITIALS] with your actual initials. Shell Command multipass launch [ INITIALS ] -vm multipass shell [ INITIALs ] -vm","title":"Lab Summary"},{"location":"module-support/vm/#1-deploy-signalfx-smart-agent-via-install-script-on-a-vm","text":"You will need to obtain your Access Token from the SignalFx UI. You can find your Access Token by clicking on your profile icon on the top right of the SignalFx UI. Then select Organization Settings \u2192 Access Tokens . Expand the Default token, then click on Show Token to expose your token. Later in the lab you can come back here and click the Copy button which will copy it to your clipboard so you can paste it when you need to provide an access token in the lab. You will also need to obtain the name of the Realm for your SignalFx account. Click on the profile icon again, but this time select 'My Profile'. The Ream can be found in the middle of the page within the Organizations section. In this example it is us1 . SignalFx maintains a shell script to install on supported distributions. Copy the script below and replace $REALM and $ACCESS_TOKEN with the values found in previous screen: Shell Command curl -sSL https://dl.signalfx.com/signalfx-agent.sh > /tmp/signalfx-agent.sh sudo sh /tmp/signalfx-agent.sh --realm $REALM -- $ACCESS_TOKEN Once the installation is complete check the status of the agent. Shell Command signalfx-agent status Output SignalFx Agent version: 5.1.2 Agent uptime: 4s Observers active: host Active Monitors: 9 Configured Monitors: 9 Discovered Endpoint Count: 16 Bad Monitor Config: None Global Dimensions: {host: as-k3s} GlobalSpanTags: map[] Datapoints sent (last minute): 0 Datapoints failed (last minute): 0 Datapoints overwritten (total): 0 Events Sent (last minute): 0 Trace Spans Sent (last minute): 0 Trace Spans overwritten (total): 0 Additional status commands: signalfx-agent status config - show resolved config in use by agent signalfx-agent status endpoints - show discovered endpoints signalfx-agent status monitors - show active monitors signalfx-agent status all - show everything Important Make a note of the value displayed for host in the Global Dimensions section of the output, as you need this later!","title":"1. Deploy SignalFx Smart Agent via install script on a VM"},{"location":"module-support/vm/#2-confirm-the-smart-agent-is-working-and-sending-data","text":"To see the Metrics that the Smart Agent is sending to SignalFx, please goto the SignalFX UI, and select Infrastructure \u2192 Hosts to see the lists of hosts. Here you see a list of the Nodes that have an Smart Agent installed and are reporting into SignalFx. Make sure you see your Multipass or AWS/EC2 instance in the list of hosts. (The hostname from the previous section) You can also set a filter for just your instance by selecting the host: attribute, followed by picking the name of your host from the drop down list. Click on the link to your host from the list, this wil take you to the overview page of your host. Make sure you have the SYSTEM METRIC tab selected. Here you can see various charts that relate to the health of your host, like CPU & Memory Used%, Disk I/O and many more. You can also see the list of services running on your host by selecting the PROCESSES tab. Take a moment to explore the various charts and the Processes list.","title":"2. Confirm the Smart Agent is working and sending data"},{"location":"monitoring-as-code/","text":"Monitoring as Code - Lab Summary \u00b6 Use Terraform 1 to manage SignalFx Dashboards and Detectors Initialize the Terraform SignalFx Provider 2 . Run Terraform to create SignalFx detectors and dashboards from code using the SignalFx Terraform Provider. See how Terraform can also delete detectors and dashboards. 1. Initial setup \u00b6 Remaining in your Multipass or AWS/EC2 instance from the Smart Agent module, change into the signalfx-jumpstart directory Shell Command cd ~/signalfx-jumpstart The environment variables needed should already be set from Deploy the Smart Agent in K3s . If not, create the following environment variables to use in the Terraform steps below Shell Command export ACCESS_TOKEN= SIGNALFX_ACCESS_TOKEN export REALM= REALM e.g. us1 export PREFIX=[$(cat /dev/urandom | base64 | tr -dc 'A-Z' | head -c4)] Initialize Terraform and upgrade to the latest version of the SignalFx Terraform Provider Upgrading the SignalFx Terraform Provider You will need to run this command each time a new version of the SignalFx Terraform Provider is released. You can track the releases on GitHub . Shell Command terraform init -upgrade Output Upgrading modules... - aws in modules/aws - azure in modules/azure - docker in modules/docker - gcp in modules/gcp - host in modules/host - kubernetes in modules/kubernetes - parent_child_dashboard in modules/dashboards/parent - pivotal in modules/pivotal - usage_dashboard in modules/dashboards/usage Initializing the backend... Initializing provider plugins... - Checking for available provider plugins... - Downloading plugin for provider \"signalfx\" (terraform-providers/signalfx) 4.18.6... The following providers do not have any version constraints in configuration, so the latest version was installed. To prevent automatic upgrades to new major versions that may contain breaking changes, it is recommended to add version = \"...\" constraints to the corresponding provider blocks in configuration, with the constraint strings suggested below. * provider.signalfx: version = \"~> 4.18\" Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing and popular service providers as well as custom in-house solutions. Configuration files describe to Terraform the components needed to run a single application or your entire datacenter. Terraform generates an execution plan describing what it will do to reach the desired state, and then executes it to build the described infrastructure. As the configuration changes, Terraform is able to determine what changed and create incremental execution plans which can be applied. The infrastructure Terraform can manage includes low-level components such as compute instances, storage, and networking, as well as high-level components such as DNS entries, SaaS features, etc. \u21a9 A provider is responsible for understanding API interactions and exposing resources. Providers generally are an IaaS (e.g. Alibaba Cloud, AWS, GCP, Microsoft Azure, OpenStack), PaaS (e.g. Heroku), or SaaS services (e.g. SignalFx, Terraform Cloud, DNSimple, Cloudflare). \u21a9","title":"Initial Setup"},{"location":"monitoring-as-code/#monitoring-as-code-lab-summary","text":"Use Terraform 1 to manage SignalFx Dashboards and Detectors Initialize the Terraform SignalFx Provider 2 . Run Terraform to create SignalFx detectors and dashboards from code using the SignalFx Terraform Provider. See how Terraform can also delete detectors and dashboards.","title":"Monitoring as Code - Lab Summary"},{"location":"monitoring-as-code/#1-initial-setup","text":"Remaining in your Multipass or AWS/EC2 instance from the Smart Agent module, change into the signalfx-jumpstart directory Shell Command cd ~/signalfx-jumpstart The environment variables needed should already be set from Deploy the Smart Agent in K3s . If not, create the following environment variables to use in the Terraform steps below Shell Command export ACCESS_TOKEN= SIGNALFX_ACCESS_TOKEN export REALM= REALM e.g. us1 export PREFIX=[$(cat /dev/urandom | base64 | tr -dc 'A-Z' | head -c4)] Initialize Terraform and upgrade to the latest version of the SignalFx Terraform Provider Upgrading the SignalFx Terraform Provider You will need to run this command each time a new version of the SignalFx Terraform Provider is released. You can track the releases on GitHub . Shell Command terraform init -upgrade Output Upgrading modules... - aws in modules/aws - azure in modules/azure - docker in modules/docker - gcp in modules/gcp - host in modules/host - kubernetes in modules/kubernetes - parent_child_dashboard in modules/dashboards/parent - pivotal in modules/pivotal - usage_dashboard in modules/dashboards/usage Initializing the backend... Initializing provider plugins... - Checking for available provider plugins... - Downloading plugin for provider \"signalfx\" (terraform-providers/signalfx) 4.18.6... The following providers do not have any version constraints in configuration, so the latest version was installed. To prevent automatic upgrades to new major versions that may contain breaking changes, it is recommended to add version = \"...\" constraints to the corresponding provider blocks in configuration, with the constraint strings suggested below. * provider.signalfx: version = \"~> 4.18\" Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing and popular service providers as well as custom in-house solutions. Configuration files describe to Terraform the components needed to run a single application or your entire datacenter. Terraform generates an execution plan describing what it will do to reach the desired state, and then executes it to build the described infrastructure. As the configuration changes, Terraform is able to determine what changed and create incremental execution plans which can be applied. The infrastructure Terraform can manage includes low-level components such as compute instances, storage, and networking, as well as high-level components such as DNS entries, SaaS features, etc. \u21a9 A provider is responsible for understanding API interactions and exposing resources. Providers generally are an IaaS (e.g. Alibaba Cloud, AWS, GCP, Microsoft Azure, OpenStack), PaaS (e.g. Heroku), or SaaS services (e.g. SignalFx, Terraform Cloud, DNSimple, Cloudflare). \u21a9","title":"1. Initial setup"},{"location":"monitoring-as-code/plan-and-apply/","text":"Plan and Apply Terraform \u00b6 1. Create an execution plan \u00b6 Review the execution plan. Shell Command terraform plan -var = \"access_token= $ACCESS_TOKEN \" -var = \"realm= $REALM \" -var = \"sfx_prefix= $PREFIX \" If the plan executes successfully, we can go ahead and apply: 2. Apply actions from execution plan \u00b6 Shell Command terraform apply -var = \"access_token= $ACCESS_TOKEN \" -var = \"realm= $REALM \" -var = \"sfx_prefix= $PREFIX \" Validate that the detectors were created, under the ALERTS \u2192 Detectors . They will be prefixed by a unique string that was initialized in the setup. To check the prefix value run: Shell Command echo ${ PREFIX } You will see a list of the new detectors and you can search for the prefix that was output from above. 3. Destroy all your hard work \u00b6 Destroy all Detectors and Dashboards that were previously applied. Shell Command terraform destroy -var = \"access_token= $ACCESS_TOKEN \" -var = \"realm= $REALM \" Validate all the detectors have been removed by navigating to ALERTS \u2192 Detectors","title":"Plan, Apply and Destroy"},{"location":"monitoring-as-code/plan-and-apply/#plan-and-apply-terraform","text":"","title":"Plan and Apply Terraform"},{"location":"monitoring-as-code/plan-and-apply/#1-create-an-execution-plan","text":"Review the execution plan. Shell Command terraform plan -var = \"access_token= $ACCESS_TOKEN \" -var = \"realm= $REALM \" -var = \"sfx_prefix= $PREFIX \" If the plan executes successfully, we can go ahead and apply:","title":"1. Create an execution plan"},{"location":"monitoring-as-code/plan-and-apply/#2-apply-actions-from-execution-plan","text":"Shell Command terraform apply -var = \"access_token= $ACCESS_TOKEN \" -var = \"realm= $REALM \" -var = \"sfx_prefix= $PREFIX \" Validate that the detectors were created, under the ALERTS \u2192 Detectors . They will be prefixed by a unique string that was initialized in the setup. To check the prefix value run: Shell Command echo ${ PREFIX } You will see a list of the new detectors and you can search for the prefix that was output from above.","title":"2. Apply actions from execution plan"},{"location":"monitoring-as-code/plan-and-apply/#3-destroy-all-your-hard-work","text":"Destroy all Detectors and Dashboards that were previously applied. Shell Command terraform destroy -var = \"access_token= $ACCESS_TOKEN \" -var = \"realm= $REALM \" Validate all the detectors have been removed by navigating to ALERTS \u2192 Detectors","title":"3. Destroy all your hard work"},{"location":"resources/","text":"Additional Splunk for DevOps Resources \u00b6 Below are helpful resources about Splunk and the DevOps use case. Topics covered are SignalFx, VictorOps, OpenTelemetry, Observability and Incident Response. Documentation \u00b6 SignalFx SignalFx Platform Docs SignalFx SignalFx API Docs VictorOps VictorOps Platform Docs VictorOps VictorOps API Docs Blog Posts \u00b6 SignalFx \"How SignalFx Does Site Reliability Engineering (SRE)\" OTEL \"OpenTelemetry for Business Continuity\" SignalFx \"In Observability RED is the new Black\" VictorOps \"Alerts to Incident Response in Three Easy Steps\" SignalFx \"Application Performance Redefined: Meet the New SignalFx Microservices APM\" Splunk \"Splunk is Lambda Ready: Announcing a New Partnership with AWS\" VictorOps \"Extended Hands and Extended Trials for Incident Response\" SignalFx \"Supporting APM for .NET Applications\" OTEL \"OpenTelemetry Consolidates Data for Observability\" Observability \"Using Observability as a Proxy for User Happiness\" Webinars & Podcast \u00b6 APM \"Future of Microservices and APM\" DevOps \"DevOps from the Top Podcast\" Observability \"Oh my! SRE, AIOps and Observe-a-what?\" Observability \"On-Demand Observability Demo\" DevOps \"Dissecting DevOps PlayList\"","title":"Links"},{"location":"resources/#additional-splunk-for-devops-resources","text":"Below are helpful resources about Splunk and the DevOps use case. Topics covered are SignalFx, VictorOps, OpenTelemetry, Observability and Incident Response.","title":"Additional Splunk for DevOps Resources"},{"location":"resources/#documentation","text":"SignalFx SignalFx Platform Docs SignalFx SignalFx API Docs VictorOps VictorOps Platform Docs VictorOps VictorOps API Docs","title":"Documentation"},{"location":"resources/#blog-posts","text":"SignalFx \"How SignalFx Does Site Reliability Engineering (SRE)\" OTEL \"OpenTelemetry for Business Continuity\" SignalFx \"In Observability RED is the new Black\" VictorOps \"Alerts to Incident Response in Three Easy Steps\" SignalFx \"Application Performance Redefined: Meet the New SignalFx Microservices APM\" Splunk \"Splunk is Lambda Ready: Announcing a New Partnership with AWS\" VictorOps \"Extended Hands and Extended Trials for Incident Response\" SignalFx \"Supporting APM for .NET Applications\" OTEL \"OpenTelemetry Consolidates Data for Observability\" Observability \"Using Observability as a Proxy for User Happiness\"","title":"Blog Posts"},{"location":"resources/#webinars-podcast","text":"APM \"Future of Microservices and APM\" DevOps \"DevOps from the Top Podcast\" Observability \"Oh my! SRE, AIOps and Observe-a-what?\" Observability \"On-Demand Observability Demo\" DevOps \"Dissecting DevOps PlayList\"","title":"Webinars &amp; Podcast"},{"location":"resources/faq/","text":"Frequently Asked Questions \u00b6 A collection of the common questions and their answers associated with Observability, DevOps, Incident Response, SignalFx and VictorOps. Q: Alerts v. Incident Response v. Incident Management \u00b6 A: Alerts, Incident Response and Incident Management are related functions. Together they comprise the incident response and resolution process. Monitoring and Observability tools send alerts to incident response platforms. Those platforms take a collection of alerts and correlate them into incidents. Those incidents are recorded into incident management (ITSM) platforms for record. Alerts are the trigger that something has happened, and provide context to an incident. Incidents consist of the alert payload, all activity associated with the incident from the time it was created, and the on-call policies to be followed. ITSM is the system of record for incidents that are active and after they have been resolved. All these components are necessary for successful incident response and management practices. VictorOps Q: Is Observability Monitoring \u00b6 A: The key difference between Monitoring and Observability is the difference between \u201cknown knowns\u201d and \u201cunknown knowns\u201d respectively. In monitoring the operator generally has prior knowledge of the architecture and elements in their system. They can reliably predict the relationship between elements, and their associated metadata. Monitoring is good for stateful infrastructure that is not frequently changed. Observability is for systems where the operators ability to predict and trace all elements in the system and their relationships is limited. Observability is a set of practices and technology, which include traditional monitoring metrics. These practices and technologies combined give the operator the ability to understand ephemeral and highly complex environments without prior knowledge of all elements of a system. Observability technology can also account for fluctuations in the environment, and variation in metadata (cardinality) better than traditional monitoring which is more static. SignalFx Q: What are Traces and Spans \u00b6 A: Traces and spans, combined with metrics and logs, make up the core types of data that feed modern Observability tools. They all have specific elements and functions, but work well together. Because microservices based architectures are distributed, transactions in the system touch multiple services before completing. This makes accurately pinpointing the location of an issue difficult. Traces are a method for tracking the full path of a request through all the services in a distributed system. Spans are the timed operations in each service. Traces are the connective tissue for the spans and together they give more detail on individual service processes. While metrics give a good snapshot of the health of a system, and logs give depth when investigating issues, traces and spans help navigate operators to the source of issues with greater context. This saves time when investigating incidents, and supports the increasing complexity of modern architectures. SignalFx Q: What is the Sidecar Pattern \u00b6 A: The sidecar pattern is a design pattern for having related services contected directly by infrastructure. Related services can be adding functionality or supporting the application logic they are connected to. It is used heavily as a method for deploying agents asscoiated with the management plan along with the application service they support. In Observability the sidecare serices are the application logic, and the agent collecting data from that service. The setup requires two containers one with the application service, and one running the agent. The containers share a pod, and resources such as disk, network, and namespace. They are also deployed together and share the same lifecycle. SignalFx","title":"FAQ"},{"location":"resources/faq/#frequently-asked-questions","text":"A collection of the common questions and their answers associated with Observability, DevOps, Incident Response, SignalFx and VictorOps.","title":"Frequently Asked Questions"},{"location":"resources/faq/#q-alerts-v-incident-response-v-incident-management","text":"A: Alerts, Incident Response and Incident Management are related functions. Together they comprise the incident response and resolution process. Monitoring and Observability tools send alerts to incident response platforms. Those platforms take a collection of alerts and correlate them into incidents. Those incidents are recorded into incident management (ITSM) platforms for record. Alerts are the trigger that something has happened, and provide context to an incident. Incidents consist of the alert payload, all activity associated with the incident from the time it was created, and the on-call policies to be followed. ITSM is the system of record for incidents that are active and after they have been resolved. All these components are necessary for successful incident response and management practices. VictorOps","title":"Q: Alerts v. Incident Response v. Incident Management"},{"location":"resources/faq/#q-is-observability-monitoring","text":"A: The key difference between Monitoring and Observability is the difference between \u201cknown knowns\u201d and \u201cunknown knowns\u201d respectively. In monitoring the operator generally has prior knowledge of the architecture and elements in their system. They can reliably predict the relationship between elements, and their associated metadata. Monitoring is good for stateful infrastructure that is not frequently changed. Observability is for systems where the operators ability to predict and trace all elements in the system and their relationships is limited. Observability is a set of practices and technology, which include traditional monitoring metrics. These practices and technologies combined give the operator the ability to understand ephemeral and highly complex environments without prior knowledge of all elements of a system. Observability technology can also account for fluctuations in the environment, and variation in metadata (cardinality) better than traditional monitoring which is more static. SignalFx","title":"Q: Is Observability Monitoring"},{"location":"resources/faq/#q-what-are-traces-and-spans","text":"A: Traces and spans, combined with metrics and logs, make up the core types of data that feed modern Observability tools. They all have specific elements and functions, but work well together. Because microservices based architectures are distributed, transactions in the system touch multiple services before completing. This makes accurately pinpointing the location of an issue difficult. Traces are a method for tracking the full path of a request through all the services in a distributed system. Spans are the timed operations in each service. Traces are the connective tissue for the spans and together they give more detail on individual service processes. While metrics give a good snapshot of the health of a system, and logs give depth when investigating issues, traces and spans help navigate operators to the source of issues with greater context. This saves time when investigating incidents, and supports the increasing complexity of modern architectures. SignalFx","title":"Q: What are Traces and Spans"},{"location":"resources/faq/#q-what-is-the-sidecar-pattern","text":"A: The sidecar pattern is a design pattern for having related services contected directly by infrastructure. Related services can be adding functionality or supporting the application logic they are connected to. It is used heavily as a method for deploying agents asscoiated with the management plan along with the application service they support. In Observability the sidecare serices are the application logic, and the agent collecting data from that service. The setup requires two containers one with the application service, and one running the agent. The containers share a pod, and resources such as disk, network, and namespace. They are also deployed together and share the same lifecycle. SignalFx","title":"Q: What is the Sidecar Pattern"},{"location":"servicebureau/billing-and-usage/","text":"Service Bureau - Lab Summary \u00b6 How to keep track of the usage of SignalFx in your organization Learn how to keep track of spend by exploring the Billing and Usage interface Creating Teams Adding notification rules to Teams Controlling Team usage 1. Understanding SignalFx engagement \u00b6 To fully understand SignalFx engagement inside your organization, click on the Settings icon on the top right of the SignalFx UI. It may also look like this From the drop down, select the Organizations Settings \u2192 Organization Overview , this will provide you with the following dashboard that shows you how your SignalFx organization is being used: On the left hand menu (not shown in the above screenshot) you will see a list of members, and in the centre, various charts that show you the number of users, teams, charts, dashboards, and dashboard groups created, as well as various growth trends. The screenshot is taken from an demonstration organization, the Workshop organization you're looking at may have less data to work with as this is cleared down after each workshop. Take a minute to explore the various charts in the Organization Overview of this Workshop instance. 2. Usage and Billing \u00b6 If you want to see what your usage is against your contract you can select the Organizations Settings \u2192 Billing and Usage from your profile icon top right of the SignalFx UI. Or the faster way is to select the Billing and Usage item from the left hand pane. This screen may take a few seconds to load whilst it calculates and pulls in the usage. 3. Understanding usage \u00b6 You will see a screen similar like the one below that will give you an overview of the current usage, the average usage and your entitlement per category : Hosts, Containers, Custom Metrics and High Resolution Metrics. For more information about about these categories please refer to Billing and Usage information . 4. Examine usage in detail \u00b6 The top chart shows you the current subscription levels per category (shown by the red arrows at the top in the screenshot below). Also, your current usage of the four catagories is displayed (shown at the red lines at the bottom of the chart). In this example you can see that there are 18 Hosts, 0 Containers and 1038 Custom Metrics and 0 High Resolution Metrics. In the bottom chart, you can see the usage per category for the current period (shown in the drop-down box on the top right of the chart). The blue line marked Average Usage indicates what SignalFx will use to calculate your average usage for the current billing period. Info As you can see from the screenshot, SignalFx does not use High Watermark or P95% for cost calculation but the actual average hourly usage, allowing you to do performance testing or Blue/Green style deployments etc. without the risk of overage charges. To get a feel for the options you can change the metric displayed by selecting the different options from the Usage Metric drop down on the left, or change the the Billing Period with the drop down on the right. Please take a minute to explore the different time periods & categories and their views. Finally, the pane on the right shows you information about your Subscription.","title":"Billing and Usage"},{"location":"servicebureau/billing-and-usage/#service-bureau-lab-summary","text":"How to keep track of the usage of SignalFx in your organization Learn how to keep track of spend by exploring the Billing and Usage interface Creating Teams Adding notification rules to Teams Controlling Team usage","title":"Service Bureau - Lab Summary"},{"location":"servicebureau/billing-and-usage/#1-understanding-signalfx-engagement","text":"To fully understand SignalFx engagement inside your organization, click on the Settings icon on the top right of the SignalFx UI. It may also look like this From the drop down, select the Organizations Settings \u2192 Organization Overview , this will provide you with the following dashboard that shows you how your SignalFx organization is being used: On the left hand menu (not shown in the above screenshot) you will see a list of members, and in the centre, various charts that show you the number of users, teams, charts, dashboards, and dashboard groups created, as well as various growth trends. The screenshot is taken from an demonstration organization, the Workshop organization you're looking at may have less data to work with as this is cleared down after each workshop. Take a minute to explore the various charts in the Organization Overview of this Workshop instance.","title":"1. Understanding SignalFx engagement"},{"location":"servicebureau/billing-and-usage/#2-usage-and-billing","text":"If you want to see what your usage is against your contract you can select the Organizations Settings \u2192 Billing and Usage from your profile icon top right of the SignalFx UI. Or the faster way is to select the Billing and Usage item from the left hand pane. This screen may take a few seconds to load whilst it calculates and pulls in the usage.","title":"2. Usage and Billing"},{"location":"servicebureau/billing-and-usage/#3-understanding-usage","text":"You will see a screen similar like the one below that will give you an overview of the current usage, the average usage and your entitlement per category : Hosts, Containers, Custom Metrics and High Resolution Metrics. For more information about about these categories please refer to Billing and Usage information .","title":"3. Understanding usage"},{"location":"servicebureau/billing-and-usage/#4-examine-usage-in-detail","text":"The top chart shows you the current subscription levels per category (shown by the red arrows at the top in the screenshot below). Also, your current usage of the four catagories is displayed (shown at the red lines at the bottom of the chart). In this example you can see that there are 18 Hosts, 0 Containers and 1038 Custom Metrics and 0 High Resolution Metrics. In the bottom chart, you can see the usage per category for the current period (shown in the drop-down box on the top right of the chart). The blue line marked Average Usage indicates what SignalFx will use to calculate your average usage for the current billing period. Info As you can see from the screenshot, SignalFx does not use High Watermark or P95% for cost calculation but the actual average hourly usage, allowing you to do performance testing or Blue/Green style deployments etc. without the risk of overage charges. To get a feel for the options you can change the metric displayed by selecting the different options from the Usage Metric drop down on the left, or change the the Billing Period with the drop down on the right. Please take a minute to explore the different time periods & categories and their views. Finally, the pane on the right shows you information about your Subscription.","title":"4. Examine usage in detail"},{"location":"servicebureau/teams/","text":"Teams - Lab Summary \u00b6 Introduction to Teams Create a Team and add members to Team Discover how you can restrict usage for Teams by creating separate access tokens and set limits. 1. Introduction to Teams \u00b6 To make sure that users see the dashboards and alerts that are relevant to them when using SignalFX, most organizations will use SignalFx's Teams feature to assign a member to one or more Teams. Ideally, this matches work related roles, for example, members of a Dev-Ops or Product Management group would be assigned to the corresponding Teams in SignalFx. When a user logs into SignalFx, they can choose which Team Dashboard will be their home page and they will typically select the page for their primary role. In the example below, the user is a member of the Development, Operations and Product Management Teams, and is currently viewing the Dashboard for the Operations Team. This Dashboard has specific Dashboard Groups for NGINX, Infra and K8s assigned but any Dashboard Group can be linked to a Teams Dashboard. They can use the menu along the top left to quickly navigate between their allocated teams, or they can use the ALL TEAMS dropdown on the right to select specific Team Dashboards, as well as quickly accessing ALL DASHBOARDS using the adjacent link. Alerts can be linked to specific Teams so the Team can monitor only the Alerts they are interested in, and in the above example they currently have 1 active Critical Alert. The Description for the Team Dashboard can be customized and can include links to team specific resources (using Markdown). 2. Creating a new Team \u00b6 To work with to SignalFx's Team UI click on the Settings icon on the top right of the SignalFx UI. It may also look like this . Select the Organizations Settings \u2192 Teams tab, or select the Teams tab from the left pane. When the Team UI is selected you will be presented with the list of current Teams. To add a new Team click on the Create New Team button. This will present you with the Create New Team dialog. Create your own team by naming it [YOUR-INITIALS]-Team and add yourself by searching for your name and selecting the Add link next to your name. This should result in a dialog similar to the one below: You can remove selected users by pressing Remove or the small x . Make sure you have your group created with your initials and with yourself added as a member, then click Done . This will bring you back to the Teams list that will now show your Team and the one's created by others. Note The Teams(s) you are a member of have a grey Member icon in front of it. If no members are assigned to your Team, you should see a blue Add Members link instead of the member count, clicking on that link will get you to the Edit Team dialog where you can add yourself. This is the same dialog you get when pressing the 3 dots ... at the end of the line with your Team and selecting Edit Team The ... menu gives you the option to Edit, Join, Leave or Delete a Team (leave and join will depend on if you are currently a member). 3. Adding Notification Rules \u00b6 You can set up specific Notification rules per team, click on the NOTIFICATION POLICY tab, this will open the notification edit menu. By default the system offers you the ability to set up a general notification rule for your team. Note The Email all team members option means all members of this Team will receive an email with the Alert information, regardless of the alert type. 3.1 Adding recipients \u00b6 You can add other recipients, by clicking Add Recipient . These recipients do not need to be SignalFx users. However if you click on the link Configure a single policy for alerts of any severity you can configure every alert level independently. Different alert rules for the different alert levels can be configured, as shown in the above image. Critical and Major are using Splunk's VictorOps Incident Management solution. For the Minor alerts we send it to the Teams Slack channel and for Warning and Info we send an email. 3.2 Notification Integrations \u00b6 In addition to sending alert notifications via email, you can configure SignalFx to send alert notifications to the services shown below. Take a moment to create some notification rules for you Team. 4. Controlling a Team's usage \u00b6 If you wish to control the consumption of Hosts, Containers, Custom Metrics and High Resolution Metrics, you can create multiple Access Tokens and allocate them to different parts of your organization. Assuming you are still within the Organization Overview section, simply select the Access Tokens tab from the left pane. However to get to this section from anywhere click on the settings icon at the top right top of the page and select Organizations Settings \u2192 Access tokens The Access Tokens Interface provides an overview of your Allotments in the form of a list of Access Tokens that have been generated. Every Organization will have a Default token generated when they are first setup, but there will typically be multiple Tokens configured. Each Token is unique and can be assigned limits for the amount of Hosts, Containers, Custom Metrics and High Resolution Metrics it can consume. The Usage Status Column quickly shows if a token is above or below its assigned limits. 4.1 Creating a new token \u00b6 Let create a new token by clicking on the New Token button. This will provide you with the Name Your Access Token dialog. Enter the new name of the new Token by using your Initials e.g. RWC-Token After you press Ok, you will be taken back to the Access Token UI, here your new token should be present, among the ones created by others. If you have made an error in your naming, want to disable/enable a token or set a Token limit, click on the ellipsis ( ... ) menu button behind a token limit to open the manage token menu. If you made a typo you can use the Rename Token option to correct the name of your token. 4.2 Disabling a token \u00b6 If you need to make sure a token cannot be used to send Metrics in you can Disable a token. Click on the Disable button to Disable the token, this means the token cannot be used for sending in data to SignalFX. The line with Your Token should become greyed out to indicate that is has been Disabled as you can see in the screenshot below. Go ahead and click on the ellipsis ( ... ) menu button to Disable and Enable your token. 4.3 Manage token usage limits \u00b6 Now Lets start limiting usage by clicking on Manage Token Limit in the 3 ... menu. This will show the Manage Token Limit Dialog: In this Dialog you can set the limits per category. Please go ahead and specify the limits as follows for each usage metric: Limit Value Host Limit 5 Container Limit 15 Custom Metric Limit 20 High Resolution Metric Limit 0 For our lab use your own email address, and double check that you have the correct numbers in your dialog box as shown in the table above. Token limits are used to trigger an alert that notify one or more recipients when the usage has been above 90% of the limit for 5 minutes. To specify the recipients, click Add Recipient , then select the recipient or notification method you want to use (specifying recipients is optional but highly recommended). The severity for token alerts is always Critical. Click on Update to save your Access Tokens limits and The Alert Settings. Going above token limit When a token is at or above its limit in a usage category, new metrics for that usage category will not be stored and processed by SignalFx. This will make sure you there will be no unexpected cost due to a team sending in data without restriction. Advanced alerting If you wish to get alerts before you hit 90%, you can create additional detectors using whatever values you want. These detectors could target the Teams consuming the specific Access Tokens so they can take action before the admins need to get involved. In your company you would distribute these new Access Tokens to various teams, controlling how much information/data they can send to SignalFx. This will allow you to fine tune the way you consume your SignalFx allotment and prevent overages from happening. Congratulations! You have now have completed the Service Bureau module.","title":"Teams"},{"location":"servicebureau/teams/#teams-lab-summary","text":"Introduction to Teams Create a Team and add members to Team Discover how you can restrict usage for Teams by creating separate access tokens and set limits.","title":"Teams - Lab Summary"},{"location":"servicebureau/teams/#1-introduction-to-teams","text":"To make sure that users see the dashboards and alerts that are relevant to them when using SignalFX, most organizations will use SignalFx's Teams feature to assign a member to one or more Teams. Ideally, this matches work related roles, for example, members of a Dev-Ops or Product Management group would be assigned to the corresponding Teams in SignalFx. When a user logs into SignalFx, they can choose which Team Dashboard will be their home page and they will typically select the page for their primary role. In the example below, the user is a member of the Development, Operations and Product Management Teams, and is currently viewing the Dashboard for the Operations Team. This Dashboard has specific Dashboard Groups for NGINX, Infra and K8s assigned but any Dashboard Group can be linked to a Teams Dashboard. They can use the menu along the top left to quickly navigate between their allocated teams, or they can use the ALL TEAMS dropdown on the right to select specific Team Dashboards, as well as quickly accessing ALL DASHBOARDS using the adjacent link. Alerts can be linked to specific Teams so the Team can monitor only the Alerts they are interested in, and in the above example they currently have 1 active Critical Alert. The Description for the Team Dashboard can be customized and can include links to team specific resources (using Markdown).","title":"1. Introduction to Teams"},{"location":"servicebureau/teams/#2-creating-a-new-team","text":"To work with to SignalFx's Team UI click on the Settings icon on the top right of the SignalFx UI. It may also look like this . Select the Organizations Settings \u2192 Teams tab, or select the Teams tab from the left pane. When the Team UI is selected you will be presented with the list of current Teams. To add a new Team click on the Create New Team button. This will present you with the Create New Team dialog. Create your own team by naming it [YOUR-INITIALS]-Team and add yourself by searching for your name and selecting the Add link next to your name. This should result in a dialog similar to the one below: You can remove selected users by pressing Remove or the small x . Make sure you have your group created with your initials and with yourself added as a member, then click Done . This will bring you back to the Teams list that will now show your Team and the one's created by others. Note The Teams(s) you are a member of have a grey Member icon in front of it. If no members are assigned to your Team, you should see a blue Add Members link instead of the member count, clicking on that link will get you to the Edit Team dialog where you can add yourself. This is the same dialog you get when pressing the 3 dots ... at the end of the line with your Team and selecting Edit Team The ... menu gives you the option to Edit, Join, Leave or Delete a Team (leave and join will depend on if you are currently a member).","title":"2. Creating a new Team"},{"location":"servicebureau/teams/#3-adding-notification-rules","text":"You can set up specific Notification rules per team, click on the NOTIFICATION POLICY tab, this will open the notification edit menu. By default the system offers you the ability to set up a general notification rule for your team. Note The Email all team members option means all members of this Team will receive an email with the Alert information, regardless of the alert type.","title":"3. Adding Notification Rules"},{"location":"servicebureau/teams/#31-adding-recipients","text":"You can add other recipients, by clicking Add Recipient . These recipients do not need to be SignalFx users. However if you click on the link Configure a single policy for alerts of any severity you can configure every alert level independently. Different alert rules for the different alert levels can be configured, as shown in the above image. Critical and Major are using Splunk's VictorOps Incident Management solution. For the Minor alerts we send it to the Teams Slack channel and for Warning and Info we send an email.","title":"3.1 Adding recipients"},{"location":"servicebureau/teams/#32-notification-integrations","text":"In addition to sending alert notifications via email, you can configure SignalFx to send alert notifications to the services shown below. Take a moment to create some notification rules for you Team.","title":"3.2 Notification Integrations"},{"location":"servicebureau/teams/#4-controlling-a-teams-usage","text":"If you wish to control the consumption of Hosts, Containers, Custom Metrics and High Resolution Metrics, you can create multiple Access Tokens and allocate them to different parts of your organization. Assuming you are still within the Organization Overview section, simply select the Access Tokens tab from the left pane. However to get to this section from anywhere click on the settings icon at the top right top of the page and select Organizations Settings \u2192 Access tokens The Access Tokens Interface provides an overview of your Allotments in the form of a list of Access Tokens that have been generated. Every Organization will have a Default token generated when they are first setup, but there will typically be multiple Tokens configured. Each Token is unique and can be assigned limits for the amount of Hosts, Containers, Custom Metrics and High Resolution Metrics it can consume. The Usage Status Column quickly shows if a token is above or below its assigned limits.","title":"4. Controlling a Team's usage"},{"location":"servicebureau/teams/#41-creating-a-new-token","text":"Let create a new token by clicking on the New Token button. This will provide you with the Name Your Access Token dialog. Enter the new name of the new Token by using your Initials e.g. RWC-Token After you press Ok, you will be taken back to the Access Token UI, here your new token should be present, among the ones created by others. If you have made an error in your naming, want to disable/enable a token or set a Token limit, click on the ellipsis ( ... ) menu button behind a token limit to open the manage token menu. If you made a typo you can use the Rename Token option to correct the name of your token.","title":"4.1 Creating a new token"},{"location":"servicebureau/teams/#42-disabling-a-token","text":"If you need to make sure a token cannot be used to send Metrics in you can Disable a token. Click on the Disable button to Disable the token, this means the token cannot be used for sending in data to SignalFX. The line with Your Token should become greyed out to indicate that is has been Disabled as you can see in the screenshot below. Go ahead and click on the ellipsis ( ... ) menu button to Disable and Enable your token.","title":"4.2 Disabling a token"},{"location":"servicebureau/teams/#43-manage-token-usage-limits","text":"Now Lets start limiting usage by clicking on Manage Token Limit in the 3 ... menu. This will show the Manage Token Limit Dialog: In this Dialog you can set the limits per category. Please go ahead and specify the limits as follows for each usage metric: Limit Value Host Limit 5 Container Limit 15 Custom Metric Limit 20 High Resolution Metric Limit 0 For our lab use your own email address, and double check that you have the correct numbers in your dialog box as shown in the table above. Token limits are used to trigger an alert that notify one or more recipients when the usage has been above 90% of the limit for 5 minutes. To specify the recipients, click Add Recipient , then select the recipient or notification method you want to use (specifying recipients is optional but highly recommended). The severity for token alerts is always Critical. Click on Update to save your Access Tokens limits and The Alert Settings. Going above token limit When a token is at or above its limit in a usage category, new metrics for that usage category will not be stored and processed by SignalFx. This will make sure you there will be no unexpected cost due to a team sending in data without restriction. Advanced alerting If you wish to get alerts before you hit 90%, you can create additional detectors using whatever values you want. These detectors could target the Teams consuming the specific Access Tokens so they can take action before the admins need to get involved. In your company you would distribute these new Access Tokens to various teams, controlling how much information/data they can send to SignalFx. This will allow you to fine tune the way you consume your SignalFx allotment and prevent overages from happening. Congratulations! You have now have completed the Service Bureau module.","title":"4.3 Manage token usage limits"},{"location":"smartagent/","text":"Introduction \u00b6 Now that you have deployed a Kubernetes cluster on Amazon EKS , it's time to set up monitoring and deploy applications. We will start by deploying SignalFx SmartAgent to our Kubernetes cluster. SignalFx is the real-time cloud monitoring solution acquired by Splunk in October 2019. The workshop also introduces you to dashboards, editing and creating charts, creating detectors to fire alerts, Monitoring as Code[^3] and the SignalFx Service Bureau[^3] capabilities in SignalFx A Word About SignalFx Real-time Streaming Architecture By the end of this technical workshop you will have a good understanding of some of the key features and capabilities of the SignalFx platform.","title":"Workshop Introduction"},{"location":"smartagent/#introduction","text":"Now that you have deployed a Kubernetes cluster on Amazon EKS , it's time to set up monitoring and deploy applications. We will start by deploying SignalFx SmartAgent to our Kubernetes cluster. SignalFx is the real-time cloud monitoring solution acquired by Splunk in October 2019. The workshop also introduces you to dashboards, editing and creating charts, creating detectors to fire alerts, Monitoring as Code[^3] and the SignalFx Service Bureau[^3] capabilities in SignalFx A Word About SignalFx Real-time Streaming Architecture By the end of this technical workshop you will have a good understanding of some of the key features and capabilities of the SignalFx platform.","title":"Introduction"},{"location":"smartagent/hungry/","text":"Create a CronJob and simulate an error \u00b6 A CronJob in Kubernetes creates job on a repeatable schedule and is written in Cron format. Shell Command kubectl apply -f hungry.yaml OUTPUT cronjob.batch/hungry-job created By looking at the workload status in Kubernetes Navigator, we can see that Kubernetes has not scheduled a pod for this CronJob. We can use Kubernetes Analyzer to identify and expedite triage process. To understand the \u201cwhy\u201d behind performance anomalies, Kubernetes Navigator leverages AI-driven analytics, which automatically surfaces insights and recommendations to precisely answer, in real-time, what is causing anomalies across the entire Kubernetes cluster \u2013 nodes, pods, containers, and workloads. Click to expand Kubernetes Analyzer Kubernetes Analyzer helps us understand the \u201cwhy\u201d behind performance anomalies by leveraging AI-driven analytics, which automatically surfaces insights and recommendations to precisely answer, in real-time, what is causing anomalies across the entire Kubernetes cluster \u2013 nodes, pods, containers, and workloads.","title":"Deploy CronJob Hungry in EKS"},{"location":"smartagent/hungry/#create-a-cronjob-and-simulate-an-error","text":"A CronJob in Kubernetes creates job on a repeatable schedule and is written in Cron format. Shell Command kubectl apply -f hungry.yaml OUTPUT cronjob.batch/hungry-job created By looking at the workload status in Kubernetes Navigator, we can see that Kubernetes has not scheduled a pod for this CronJob. We can use Kubernetes Analyzer to identify and expedite triage process. To understand the \u201cwhy\u201d behind performance anomalies, Kubernetes Navigator leverages AI-driven analytics, which automatically surfaces insights and recommendations to precisely answer, in real-time, what is causing anomalies across the entire Kubernetes cluster \u2013 nodes, pods, containers, and workloads. Click to expand Kubernetes Analyzer Kubernetes Analyzer helps us understand the \u201cwhy\u201d behind performance anomalies by leveraging AI-driven analytics, which automatically surfaces insights and recommendations to precisely answer, in real-time, what is causing anomalies across the entire Kubernetes cluster \u2013 nodes, pods, containers, and workloads.","title":"Create a CronJob and simulate an error"},{"location":"smartagent/imagesample/","text":"Create a Deployment simulating a failure \u00b6 Let's simulate some real-world failures. Here we are creating a sample Deployment imagesample Shell Command kubectl apply -f imagesample.yaml OUTPUT deployment.apps/imagesample created However, there is a twist: although the output message says the Deployment has been created we can see something is not right in Kubernetes Navigator Our pod is continuing to stay in Pending state. Why is this happening? Currently Kubernetes Navigator tells us something is wrong but we don't necessarily get the root cause of this. Wait till we get our logging setup with Splunk Cloud, then we will determine the root cause and fix the underlying issue.","title":"Deploy Imagesample in EKS"},{"location":"smartagent/imagesample/#create-a-deployment-simulating-a-failure","text":"Let's simulate some real-world failures. Here we are creating a sample Deployment imagesample Shell Command kubectl apply -f imagesample.yaml OUTPUT deployment.apps/imagesample created However, there is a twist: although the output message says the Deployment has been created we can see something is not right in Kubernetes Navigator Our pod is continuing to stay in Pending state. Why is this happening? Currently Kubernetes Navigator tells us something is wrong but we don't necessarily get the root cause of this. Wait till we get our logging setup with Splunk Cloud, then we will determine the root cause and fix the underlying issue.","title":"Create a Deployment simulating a failure"},{"location":"smartagent/k3s/","text":"Deploying the Smart Agent in Amazon EKS \u00b6 Use the SignalFx Helm chart to install the Smart Agent in EKS Explore your cluster in the Kubernetes Navigator 1. Obtain SignalFx Access Token \u00b6 You will need to obtain your Access Token 1 from the SignalFx UI once Kubernetes is running. You can find your Access Token by clicking on your profile icon on the top right of the SignalFx UI. Then select Organization Settings \u2192 Access Tokens . Expand the Default token, then click on Show Token to expose your token. Click the Copy button to copy to clipboard. You will also need to obtain the name of the Realm for your SignalFx account. Click on the profile icon again, but this time select My Profile . The Realm can be found in the middle of the page within the Organizations section. In this example it is us2 . 2. Use Helm to deploy agent \u00b6 Create the following variables to use in the proceeding helm install command, replacing the highlighted VARIABLE with the appropriate values. For instance, if your realm is us2 , you would run export REALM=us2 and for eu0 run export REALM=eu0 . Shell Command export ACCESS_TOKEN= ACCESS TOKEN, from organisation page export REALM= REALM e.g. us1 Install the agent using the SignalFx Helm chart. Firstly, add the SignalFx Helm chart repository to Helm. Shell Command helm repo add signalfx https://dl.signalfx.com/helm-repo && helm repo update Install the Smart Agent Helm chart with the following commands, do NOT edit this: Shell Command cd signalfx helm install \\ --set signalFxAccessToken=$ACCESS_TOKEN \\ --set clusterName=my-k8s-cluster \\ --set kubeletAPI.url=https://localhost:10250 \\ --set agentVersion=5.2.1 \\ --set signalFxRealm=$REALM \\ --set gatherDockerMetrics=false \\ signalfx-agent signalfx/signalfx-agent \\ -f values.yaml You can monitor the progress of the deployment by running kubectl get pods which should typically report a new pod is up and running after about 30 seconds. Ensure the status is reported as Running before continuing. Shell Command kubectl get pods Output NAME READY STATUS RESTARTS AGE signalfx-agent-66tvr 1/1 Running 0 7s Ensure there are no errors by tailing the logs from the Smart Agent Pod. Output should look similar to the log output shown below. Use the label set by the helm install to tail logs (You will need to press Ctrl + C to exit). Shell Command kubectl logs -l app = signalfx-agent -f Output signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Starting up agent version 5.2.1\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Watching for config file changes\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"New config loaded\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Using log level info\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Fetching host id dimensions\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Trying to get fully qualified hostname\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Using hostname sedj\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Using host id dimensions map[host:sedj kubernetes_node_uid:ea3bf9ff-3f04-4485-9702-6e7097b261dd]\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Sending datapoints to https://ingest.us0.signalfx.com/v2/datapoint\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Sending events to https://ingest.us0.signalfx.com/v2/event\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Sending trace spans to https://ingest.us0.signalfx.com/v2/trace\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Setting cluster:sedj-k8s-cluster property on host:sedj dimension\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=1 monitorType=cpu \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=2 monitorType=filesystems \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=3 monitorType=disk-io \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=4 monitorType=net-io \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=5 monitorType=load \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=6 monitorType=memory \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=7 monitorType=host-metadata \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=8 monitorType=processlist \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=9 monitorType=vmem \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=10 monitorType=kubelet-stats \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=11 monitorType=kubernetes-cluster \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=12 monitorType=signalfx-forwarder \u2502 signalfx-agent I0527 20:52:12.796150 1 leaderelection.go:242] attempting to acquire leader lease default/signalfx-agent-leader... \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=13 monitorType=kubernetes-events \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Done configuring agent\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Serving internal metrics at localhost:8095\" \u2502 signalfx-agent I0527 20:52:12.813288 1 leaderelection.go:252] successfully acquired lease default/signalfx-agent-leader \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"K8s leader is now node sedj\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"This instance is now the leader and will send events\" monitorType=kubernetes-events \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Starting K8s API resource sync\" \u2502 3. Validate metrics in the UI \u00b6 In the SignalFx UI, goto INFRASTRUCTURE \u2192 Kubernetes Navigator \u2192 Cluster Map and open the Kubernetes Navigator Cluster Map to ensure metrics are being sent. Validate that your cluster is discovered and reporting by finding your cluster To examine the health of your node, first click on the blue cross on your cluster. This will drill down to the node level. Next, open the side bar by clicking on the side bar button to open the Metrics side bar. Once it is open, you can use the slider on the side to explore the various charts relevant to your cluster/node: CPU, Memory, Network, Events etc. replace screenshot Access tokens (sometimes called org tokens) are long-lived organization-level tokens. By default, these tokens persist for 5 years, and thus are suitable for embedding into emitters that send data points over long periods of time, or for any long-running scripts that call the SignalFx API. \u21a9","title":"Deploy the Smart Agent in EKS"},{"location":"smartagent/k3s/#deploying-the-smart-agent-in-amazon-eks","text":"Use the SignalFx Helm chart to install the Smart Agent in EKS Explore your cluster in the Kubernetes Navigator","title":"Deploying the Smart Agent in Amazon EKS"},{"location":"smartagent/k3s/#1-obtain-signalfx-access-token","text":"You will need to obtain your Access Token 1 from the SignalFx UI once Kubernetes is running. You can find your Access Token by clicking on your profile icon on the top right of the SignalFx UI. Then select Organization Settings \u2192 Access Tokens . Expand the Default token, then click on Show Token to expose your token. Click the Copy button to copy to clipboard. You will also need to obtain the name of the Realm for your SignalFx account. Click on the profile icon again, but this time select My Profile . The Realm can be found in the middle of the page within the Organizations section. In this example it is us2 .","title":"1. Obtain SignalFx Access Token"},{"location":"smartagent/k3s/#2-use-helm-to-deploy-agent","text":"Create the following variables to use in the proceeding helm install command, replacing the highlighted VARIABLE with the appropriate values. For instance, if your realm is us2 , you would run export REALM=us2 and for eu0 run export REALM=eu0 . Shell Command export ACCESS_TOKEN= ACCESS TOKEN, from organisation page export REALM= REALM e.g. us1 Install the agent using the SignalFx Helm chart. Firstly, add the SignalFx Helm chart repository to Helm. Shell Command helm repo add signalfx https://dl.signalfx.com/helm-repo && helm repo update Install the Smart Agent Helm chart with the following commands, do NOT edit this: Shell Command cd signalfx helm install \\ --set signalFxAccessToken=$ACCESS_TOKEN \\ --set clusterName=my-k8s-cluster \\ --set kubeletAPI.url=https://localhost:10250 \\ --set agentVersion=5.2.1 \\ --set signalFxRealm=$REALM \\ --set gatherDockerMetrics=false \\ signalfx-agent signalfx/signalfx-agent \\ -f values.yaml You can monitor the progress of the deployment by running kubectl get pods which should typically report a new pod is up and running after about 30 seconds. Ensure the status is reported as Running before continuing. Shell Command kubectl get pods Output NAME READY STATUS RESTARTS AGE signalfx-agent-66tvr 1/1 Running 0 7s Ensure there are no errors by tailing the logs from the Smart Agent Pod. Output should look similar to the log output shown below. Use the label set by the helm install to tail logs (You will need to press Ctrl + C to exit). Shell Command kubectl logs -l app = signalfx-agent -f Output signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Starting up agent version 5.2.1\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Watching for config file changes\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"New config loaded\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Using log level info\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Fetching host id dimensions\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Trying to get fully qualified hostname\" \u2502 signalfx-agent time=\"2020-05-27T20:52:10Z\" level=info msg=\"Using hostname sedj\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Using host id dimensions map[host:sedj kubernetes_node_uid:ea3bf9ff-3f04-4485-9702-6e7097b261dd]\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Sending datapoints to https://ingest.us0.signalfx.com/v2/datapoint\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Sending events to https://ingest.us0.signalfx.com/v2/event\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Sending trace spans to https://ingest.us0.signalfx.com/v2/trace\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Setting cluster:sedj-k8s-cluster property on host:sedj dimension\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=1 monitorType=cpu \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=2 monitorType=filesystems \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=3 monitorType=disk-io \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=4 monitorType=net-io \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=5 monitorType=load \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=6 monitorType=memory \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=7 monitorType=host-metadata \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=8 monitorType=processlist \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=9 monitorType=vmem \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=10 monitorType=kubelet-stats \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=11 monitorType=kubernetes-cluster \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=12 monitorType=signalfx-forwarder \u2502 signalfx-agent I0527 20:52:12.796150 1 leaderelection.go:242] attempting to acquire leader lease default/signalfx-agent-leader... \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Creating new monitor\" discoveryRule= monitorID=13 monitorType=kubernetes-events \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Done configuring agent\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Serving internal metrics at localhost:8095\" \u2502 signalfx-agent I0527 20:52:12.813288 1 leaderelection.go:252] successfully acquired lease default/signalfx-agent-leader \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"K8s leader is now node sedj\" \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"This instance is now the leader and will send events\" monitorType=kubernetes-events \u2502 signalfx-agent time=\"2020-05-27T20:52:12Z\" level=info msg=\"Starting K8s API resource sync\" \u2502","title":"2. Use Helm to deploy agent"},{"location":"smartagent/k3s/#3-validate-metrics-in-the-ui","text":"In the SignalFx UI, goto INFRASTRUCTURE \u2192 Kubernetes Navigator \u2192 Cluster Map and open the Kubernetes Navigator Cluster Map to ensure metrics are being sent. Validate that your cluster is discovered and reporting by finding your cluster To examine the health of your node, first click on the blue cross on your cluster. This will drill down to the node level. Next, open the side bar by clicking on the side bar button to open the Metrics side bar. Once it is open, you can use the slider on the side to explore the various charts relevant to your cluster/node: CPU, Memory, Network, Events etc. replace screenshot Access tokens (sometimes called org tokens) are long-lived organization-level tokens. By default, these tokens persist for 5 years, and thus are suitable for embedding into emitters that send data points over long periods of time, or for any long-running scripts that call the SignalFx API. \u21a9","title":"3. Validate metrics in the UI"},{"location":"smartagent/mario/","text":"Create a Namespace called mario \u00b6 Now that we have eyes on the cluster, let's have some fun! Namespaces are a way to divide cluster resources between multiple users, teams, projects or applications. kubectl create ns mario Deploy Super Mario pod \u00b6 Deploy a Kubernetes Deployment named mario and a Service called mario-external Shell Command kubectl -n mario apply -f mario.yaml Output deployment.apps/mario created service/mario-external created You can access your mario pod service by accessing via the load balancer. It takes a few minutes to create Elastic Load Balancers and hooking up networking. Shell Command kubectl -n mario get svc Output NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE mario-external LoadBalancer 10.100.68.49 aa573ed93a4b645d686ea9a8af5f9eb2-348616612.us-west-2.elb.amazonaws.com 80:31157/TCP 25s Copy the EXTERNAL-IP from the output of the above acommand. Point your browser at http:// EXTERNAL-IP and take a few minutes to enjoy the fruits of your labour!","title":"Deploy Mario in EKS"},{"location":"smartagent/mario/#create-a-namespace-called-mario","text":"Now that we have eyes on the cluster, let's have some fun! Namespaces are a way to divide cluster resources between multiple users, teams, projects or applications. kubectl create ns mario","title":"Create a Namespace called mario"},{"location":"smartagent/mario/#deploy-super-mario-pod","text":"Deploy a Kubernetes Deployment named mario and a Service called mario-external Shell Command kubectl -n mario apply -f mario.yaml Output deployment.apps/mario created service/mario-external created You can access your mario pod service by accessing via the load balancer. It takes a few minutes to create Elastic Load Balancers and hooking up networking. Shell Command kubectl -n mario get svc Output NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE mario-external LoadBalancer 10.100.68.49 aa573ed93a4b645d686ea9a8af5f9eb2-348616612.us-west-2.elb.amazonaws.com 80:31157/TCP 25s Copy the EXTERNAL-IP from the output of the above acommand. Point your browser at http:// EXTERNAL-IP and take a few minutes to enjoy the fruits of your labour!","title":"Deploy Super Mario pod"},{"location":"smartagent/nginx/","text":"Deploying NGINX in Amazoon EKS - Lab Summary \u00b6 Deploy a NGINX ReplicaSet into your Kubernetes cluster and confirm the auto discovery of your NGINX deployment. Run a benchmark test to create metrics and confirm them streaming into SignalFx! 1. Start your NGINX \u00b6 This deployment of NGINX has been configured to use Kubernetes pod annotations to tell the Smart Agent how to monitor the service. This is achieved by defining the port and monitor type to use for monitoring the NGINX service e.g. Example Annotation agent.signalfx.com/monitorType.80: \"collectd/nginx\" Verify the number of pods running in the SignalFx UI by selecting the WORKLOADS tab. This should give you an overview of the workloads on your cluster. Note the single agent container running per node among the default Kubernetes pods. This single container will monitor all the pods and services being deployed on this node! Now switch back to the default cluster node view by selecting the MAP tab and select your cluster again. Shell Command cd apps/nginx 2. Create NGINX deployment \u00b6 Create the NGINX configmap 1 using the nginx.conf file: Shell Command kubectl create configmap nginxconfig --from-file=nginx.conf Output configmap/nginxconfig created Then create the deployment: Shell Command kubectl create -f nginx-deployment.yaml Output deployment.apps/nginx created service/nginx created Validate the deployment has been successful and that the NGINX pods are running. If you have the SignalFx UI open you should see new Pods being started and containers being deployed. It should only take around 20 seconds for the pods to transition into a Running state. In the SignalFx UI you will have a cluster that looks like below: If you select the WORKLOADS tab again you will now see that there is a new ReplicaSet and a deployment added for NGINX: Let's validate this in your shell as well: Shell Command kubectl get pods Output NAME READY STATUS RESTARTS AGE signalfx-agent-7mljv 1/1 Running 0 87m nginx-7554f6c668-pdjkp 1/1 Running 0 65s nginx-7554f6c668-pddfj 1/1 Running 0 65s nginx-7554f6c668-ggblg 1/1 Running 0 65s nginx-7554f6c668-mjtsh 1/1 Running 0 65s Run kubectl get svc and make note of the EXTERNAL-IP address that is allocated to the NGINX service. Shell Command kubectl get svc Output NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 9m3s nginx NodePort 10.110.36.62 afd8d9c787298405aa1d3970d7a82af2-1379443525.us-west-2.elb.amazonaws.com 80:30995/TCP 8s Install Siege Benchmark \u00b6 Shell Command sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo yum-config-manager --enable epel sudo yum install siege 3. Run Siege Benchmark \u00b6 Using the NGINX EXTERNAL-IP address reported from above, use the Siege 2 Load Testing command to generate some traffic to light up your SignalFx NGINX dashboards. Run this a couple of times! Shell Command siege -b -r 50 -c 20 --no-parser http:// EXTERNAL-IP / 1>/dev/null Output ** SIEGE 4.0.5 ** Preparing 20 concurrent users for battle. The server is now under siege... Transactions: 1000 hits Availability: 100.00 % Elapsed time: 1.17 secs Data transferred: 20.05 MB Response time: 0.02 secs Transaction rate: 854.70 trans/sec Throughput: 17.14 MB/sec Concurrency: 19.77 Successful transactions: 1000 Failed transactions: 0 Longest transaction: 0.16 Shortest transaction: 0.01 Validate you are seeing metrics in the UI by going to Dashboards \u2192 NGINX \u2192 NGINX Servers A ConfigMap is an API object used to store non-confidential data in key-value pairs. Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume. A ConfigMap allows you to decouple environment-specific configuration from your container images, so that your applications are easily portable. \u21a9 What is Siege? \u21a9","title":"Deploy NGINX in EKS"},{"location":"smartagent/nginx/#deploying-nginx-in-amazoon-eks-lab-summary","text":"Deploy a NGINX ReplicaSet into your Kubernetes cluster and confirm the auto discovery of your NGINX deployment. Run a benchmark test to create metrics and confirm them streaming into SignalFx!","title":"Deploying NGINX in Amazoon EKS - Lab Summary"},{"location":"smartagent/nginx/#1-start-your-nginx","text":"This deployment of NGINX has been configured to use Kubernetes pod annotations to tell the Smart Agent how to monitor the service. This is achieved by defining the port and monitor type to use for monitoring the NGINX service e.g. Example Annotation agent.signalfx.com/monitorType.80: \"collectd/nginx\" Verify the number of pods running in the SignalFx UI by selecting the WORKLOADS tab. This should give you an overview of the workloads on your cluster. Note the single agent container running per node among the default Kubernetes pods. This single container will monitor all the pods and services being deployed on this node! Now switch back to the default cluster node view by selecting the MAP tab and select your cluster again. Shell Command cd apps/nginx","title":"1. Start your NGINX"},{"location":"smartagent/nginx/#2-create-nginx-deployment","text":"Create the NGINX configmap 1 using the nginx.conf file: Shell Command kubectl create configmap nginxconfig --from-file=nginx.conf Output configmap/nginxconfig created Then create the deployment: Shell Command kubectl create -f nginx-deployment.yaml Output deployment.apps/nginx created service/nginx created Validate the deployment has been successful and that the NGINX pods are running. If you have the SignalFx UI open you should see new Pods being started and containers being deployed. It should only take around 20 seconds for the pods to transition into a Running state. In the SignalFx UI you will have a cluster that looks like below: If you select the WORKLOADS tab again you will now see that there is a new ReplicaSet and a deployment added for NGINX: Let's validate this in your shell as well: Shell Command kubectl get pods Output NAME READY STATUS RESTARTS AGE signalfx-agent-7mljv 1/1 Running 0 87m nginx-7554f6c668-pdjkp 1/1 Running 0 65s nginx-7554f6c668-pddfj 1/1 Running 0 65s nginx-7554f6c668-ggblg 1/1 Running 0 65s nginx-7554f6c668-mjtsh 1/1 Running 0 65s Run kubectl get svc and make note of the EXTERNAL-IP address that is allocated to the NGINX service. Shell Command kubectl get svc Output NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 9m3s nginx NodePort 10.110.36.62 afd8d9c787298405aa1d3970d7a82af2-1379443525.us-west-2.elb.amazonaws.com 80:30995/TCP 8s","title":"2. Create NGINX deployment"},{"location":"smartagent/nginx/#install-siege-benchmark","text":"Shell Command sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo yum-config-manager --enable epel sudo yum install siege","title":"Install Siege Benchmark"},{"location":"smartagent/nginx/#3-run-siege-benchmark","text":"Using the NGINX EXTERNAL-IP address reported from above, use the Siege 2 Load Testing command to generate some traffic to light up your SignalFx NGINX dashboards. Run this a couple of times! Shell Command siege -b -r 50 -c 20 --no-parser http:// EXTERNAL-IP / 1>/dev/null Output ** SIEGE 4.0.5 ** Preparing 20 concurrent users for battle. The server is now under siege... Transactions: 1000 hits Availability: 100.00 % Elapsed time: 1.17 secs Data transferred: 20.05 MB Response time: 0.02 secs Transaction rate: 854.70 trans/sec Throughput: 17.14 MB/sec Concurrency: 19.77 Successful transactions: 1000 Failed transactions: 0 Longest transaction: 0.16 Shortest transaction: 0.01 Validate you are seeing metrics in the UI by going to Dashboards \u2192 NGINX \u2192 NGINX Servers A ConfigMap is an API object used to store non-confidential data in key-value pairs. Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume. A ConfigMap allows you to decouple environment-specific configuration from your container images, so that your applications are easily portable. \u21a9 What is Siege? \u21a9","title":"3. Run Siege Benchmark"},{"location":"smartagent/prep/","text":"Get Data In - Lab Summary \u00b6 Download the Workshop Start a Multipass 1 or AWS/EC2 instance Deploy the SignalFx Smart Agent 2 in K3s Validate Kubernetes 3 K3s cluster is visible in Kubernetes Navigator Deploy a NGINX 4 ReplicaSet in K3s Validate NGNIX metrics are flowing 1. Module Pre-requisites \u00b6 Running Locally Multipass Install Multipass for your operating system. Make sure you are using at least version 1.2.0 . On a Mac you can also install via Homebrew e.g. brew cask install multipass Struggling with Multipass? Ask your instructor(s) for access to a pre-provisioned AWS/EC2 instance, you can then ignore the rest of this preparation lab and go straight to the next lab Deploying the Smart Agent in Kubernetes (K3s) . Running in AWS AWS/EC2 Instance Install Terraform for your operating system. Please make sure it is version 0.12.18 or above. On a Mac you can also install via Homebrew e.g. brew install terraform . This will get around Mac OS Catalina security. 2. Download Observability Workshop \u00b6 Regardless if you are running this lab locally or if you are going to create your own AWS/EC2 Instance you need to download the Observability Workshop zip file locally, unzip the file, rename it and cd into the directory. Linux/Mac OS WSVERSION = 1 .33 mkdir cloud-init curl -s \\ https://raw.githubusercontent.com/signalfx/observability-workshop/v $WSVERSION /cloud-init/k3s.yaml \\ -o cloud-init/k3s.yaml export INSTANCE = $( cat /dev/urandom | base64 | tr -dc 'a-z' | head -c4 ) Windows Info Download the zip by clicking on the following URL https://github.com/signalfx/observability-workshop/archive/v1.33.zip . Once downloaded, unzip the the file and rename it to workshop . Then, from the command prompt change into that directory and run $INSTANCE = (\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\".tochararray() | sort {Get-Random})[0..3] -join '' If you are using your own AWS/EC2 instance please skip to 3. Launch Instance section and select the Launch AWS/EC2 instance tab 3. Launch Instance \u00b6 Launch Multipass instance In this section you will build and launch the Multipass instance which will run the Kubernetes (K3s) environment that you will use in multiple labs. For \u00b5APM module we use the Hot R.O.D 5 application to emit Traces/Spans for SignalFx \u00b5APM. Launch your instance with: multipass launch \\ --name ${ INSTANCE } \\ --cloud-init cloud-init/k3s.yaml Once the instance has been successfully created (this can take several minutes), shell into it. Shell Command multipass shell ${ INSTANCE } Output \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u255d \u255a\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2554\u255d \u255a\u2588\u2588\u2557 \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2588\u2588\u2557 \u2588\u2588\u2554\u255d \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2557 \u2588\u2588\u2554\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d To run a command as administrator (user \"root\"), use \"sudo <command>\". See \"man sudo_root\" for details Waiting for cloud-init status... Your instance is ready! ubuntu@vmpe-k3s:~$ Once your instance presents you with the Splunk logo, you have completed the preparation for your Multipass instance and can go directly to the next lab Deploy the Smart Agent in K3s . Launch AWS/EC2 instance In this section you will use terraform to build an AWS/EC2 instance in your favorite AWS region and will automatically deploy the Kubernetes (K3s) environment that you will use in this Workshop. AWS Access Keys You will need access to an AWS account to obtain both AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY . Minimum requirements For the \u00b5APM module we are using the Hot R.O.D. application. The minimum requirements are: Hot R.O.D AWS/EC2 Instance min. requirements: t2.micro 1 vCPU, 8Gb Disk, 1Gb Memory Prepare Terraform The first step is to go into the sub-directory where the Terraform files are located and initialise Terraform and upgrade the AWS Terraform Provider. Shell Command cd ec2 terraform init -upgrade Output ~/workshop/ec2$ terraform init -upgrade Initializing the backend... Initializing provider plugins... - Checking for available provider plugins... - Downloading plugin for provider \"aws\" (hashicorp/aws) 2.60.0... The following providers do not have any version constraints in configuration, so the latest version was installed. To prevent automatic upgrades to new major versions that may contain breaking changes, it is recommended to add version = \"...\" constraints to the corresponding provider blocks in configuration, with the constraint strings suggested below. * provider.aws: version = \"~> 2.60\" Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. Create AWS/EC2 Instance Creating the AWS/EC2 instance is done in two steps, a planning phase and an apply phase. The planning phase will validate the Terraform scripts and check what changes it will make to your AWS environment. The apply phase will actually create the instance. First, you need to create environment variables for your AWS access keys. Shell Command export AWS_ACCESS_KEY_ID = \" YOUR_AWS_ACCESS_KEY_ID \" export AWS_SECRET_ACCESS_KEY = \" YOUR_AWS_SECRET_ACCESS_KEY \" echo $AWS_ACCESS_KEY_ID $AWS_SECRET_ACCESS_KEY Output ID: Axxxxxxxxxxxxxxxxy, KEY: Axxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb Once you have confirmed that you have set you AWS_SECRET_ACCESS_KEY_ID & AWS_SECRET_ACCESS_KEY correctly, you can start with the planning phase. Important Desired AWS Region : (Any AWS region by name, for example us-west-2 ) Please remember these values as you will need them again for the planning phase and when you use Terraform to destroy your AWS/EC2 instance. As we only wish to provide the input once, we are going to capture the output in a .out file that we can use for the apply step. Please provide your initials for the output file as indicated. Shell Command terraform plan -var = \"aws_instance_count=1\" -var = \"instance_type=1\" -out = observability-plan.out Enter your desired AWS Region where you wish to run the AWS/EC2 instance e.g. us-west-2 Example var.aws_region Provide the desired region Enter a value: us-west-2 Output Refreshing Terraform state in-memory prior to plan... The refreshed state will be used to calculate this plan, but will not be persisted to local or remote state storage. data.aws_ami.latest-ubuntu: Refreshing state... ------------------------------------------------------------------------ An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: **(BIG WALL OF AWS RELATED TEXT REMOVED)** Plan: 2 to add, 0 to change, 0 to destroy. ------------------------------------------------------------------------ This plan was saved to: observability-plan.out To perform exactly these actions, run the following command to apply: terraform apply \"observability-plan.out\" If there are no errors in the output and terraform has created your output file, you can start the apply phase of Terraform. This will create the AWS/EC2 instance. Shell Command terraform apply \"observability-plan.out\" Output ws_security_group.instance: Creating... aws_security_group.instance: Creation complete after 2s [id=sg-0459afecae5953b51] aws_instance.observability-instance[0]: Creating... aws_instance.observability-instance[0]: Still creating... [10s elapsed] aws_instance.observability-instance[0]: Still creating... [20s elapsed] aws_instance.observability-instance[0]: Creation complete after 23s [id=i-095a12cd39f8e2283] Apply complete! Resources: 2 added, 0 changed, 0 destroyed. The state of your infrastructure has been saved to the path below. This state is required to modify and destroy your infrastructure, so keep it safe. To inspect the complete state use the `terraform show` command. State path: terraform.tfstate Outputs: ip = [ \"YOUR IP ADDRESS\", ] Verify there are no errors and copy the ip address that you see in the green output. SSH into AWS/EC2 Instance Once the instance has been successfully created (this can take several minutes), ssh into it. In most cases your ssh client will ask you to verify the connection. Shell Command ssh ubuntu@YOUR IP ADDRESS Output The authenticity of host 'YOUR IP ADDRESS (YOUR IP ADDRESS)' can't be established. ECDSA key fingerprint is SHA256:XdqN55g0z/ER660PARM+mGqtpYpwM3333YS9Ac8Y9hLY. Are you sure you want to continue connecting (yes/no/[fingerprint])? Please confirm that you wish to continue by replying to the prompt with yes Shell Command Are you sure you want to continue connecting ( yes/no/ [ fingerprint ]) ? yes Output Warning: Permanently added 'YOUR IP ADDRESS' (ECDSA) to the list of known hosts. ubuntu@YOUR IP ADDRESS's password: To login to your instance please use the password provided by the Workshop host. Shell Command ubuntu@YOUR IP ADDRESS ' s password: PASSWORD Output \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u255d \u255a\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2554\u255d \u255a\u2588\u2588\u2557 \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2588\u2588\u2557 \u2588\u2588\u2554\u255d \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2557 \u2588\u2588\u2554\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d To run a command as administrator (user \"root\"), use \"sudo <command>\". See \"man sudo_root\" for details Waiting for cloud-init status... Your instance is ready! ubuntu@ip-172-31-41-196:~$ Once your instance presents you with the Splunk logo, make sure you see Your instance is ready! in the output. You have now completed the preparation for your AWS/EC2 instance and can go directly to the next lab Deploy the Smart Agent in K3s . 4. About SignalFx Realms \u00b6 A realm is a self-contained deployment of SignalFx in which your organization is hosted. Different realms have different API endpoints (e.g. the endpoint for sending data is ingest.us1.signalfx.com for the us1 realm, and ingest.eu0.signalfx.com for the eu0 realm). Various instructions in this Workshop include a REALM placeholder that will need to be replaced with the actual name of your realm. This realm name is shown on your profile page in SignalFx. If you do not include the realm name when specifying an endpoint, SignalFx will interpret it as pointing to the us0 realm. Multipass is a lightweight VM manager for Linux, Windows and macOS. It's designed for developers who want a fresh Ubuntu environment with a single command. It uses KVM on Linux, Hyper-V on Windows and HyperKit on macOS to run the VM with minimal overhead. It can also use VirtualBox on Windows and macOS. Multipass will fetch images for you and keep them up to date. \u21a9 The SignalFx Smart Agent gathers host performance, application, and service-level metrics from both containerized and non-container environments. The Smart Agent installs with more than 100 bundled monitors for gathering data, including Python-based plug-ins such as Mongo, Redis, and Docker. \u21a9 What is Kubernetes? \u21a9 What is NGINX? \u21a9 What is Hot R.O.D.? \u21a9","title":"Get Data In - Lab Summary"},{"location":"smartagent/prep/#get-data-in-lab-summary","text":"Download the Workshop Start a Multipass 1 or AWS/EC2 instance Deploy the SignalFx Smart Agent 2 in K3s Validate Kubernetes 3 K3s cluster is visible in Kubernetes Navigator Deploy a NGINX 4 ReplicaSet in K3s Validate NGNIX metrics are flowing","title":"Get Data In - Lab Summary"},{"location":"smartagent/prep/#1-module-pre-requisites","text":"Running Locally Multipass Install Multipass for your operating system. Make sure you are using at least version 1.2.0 . On a Mac you can also install via Homebrew e.g. brew cask install multipass Struggling with Multipass? Ask your instructor(s) for access to a pre-provisioned AWS/EC2 instance, you can then ignore the rest of this preparation lab and go straight to the next lab Deploying the Smart Agent in Kubernetes (K3s) . Running in AWS AWS/EC2 Instance Install Terraform for your operating system. Please make sure it is version 0.12.18 or above. On a Mac you can also install via Homebrew e.g. brew install terraform . This will get around Mac OS Catalina security.","title":"1. Module Pre-requisites"},{"location":"smartagent/prep/#2-download-observability-workshop","text":"Regardless if you are running this lab locally or if you are going to create your own AWS/EC2 Instance you need to download the Observability Workshop zip file locally, unzip the file, rename it and cd into the directory. Linux/Mac OS WSVERSION = 1 .33 mkdir cloud-init curl -s \\ https://raw.githubusercontent.com/signalfx/observability-workshop/v $WSVERSION /cloud-init/k3s.yaml \\ -o cloud-init/k3s.yaml export INSTANCE = $( cat /dev/urandom | base64 | tr -dc 'a-z' | head -c4 ) Windows Info Download the zip by clicking on the following URL https://github.com/signalfx/observability-workshop/archive/v1.33.zip . Once downloaded, unzip the the file and rename it to workshop . Then, from the command prompt change into that directory and run $INSTANCE = (\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\".tochararray() | sort {Get-Random})[0..3] -join '' If you are using your own AWS/EC2 instance please skip to 3. Launch Instance section and select the Launch AWS/EC2 instance tab","title":"2. Download Observability Workshop"},{"location":"smartagent/prep/#3-launch-instance","text":"Launch Multipass instance In this section you will build and launch the Multipass instance which will run the Kubernetes (K3s) environment that you will use in multiple labs. For \u00b5APM module we use the Hot R.O.D 5 application to emit Traces/Spans for SignalFx \u00b5APM. Launch your instance with: multipass launch \\ --name ${ INSTANCE } \\ --cloud-init cloud-init/k3s.yaml Once the instance has been successfully created (this can take several minutes), shell into it. Shell Command multipass shell ${ INSTANCE } Output \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u255d \u255a\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2554\u255d \u255a\u2588\u2588\u2557 \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2588\u2588\u2557 \u2588\u2588\u2554\u255d \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2557 \u2588\u2588\u2554\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d To run a command as administrator (user \"root\"), use \"sudo <command>\". See \"man sudo_root\" for details Waiting for cloud-init status... Your instance is ready! ubuntu@vmpe-k3s:~$ Once your instance presents you with the Splunk logo, you have completed the preparation for your Multipass instance and can go directly to the next lab Deploy the Smart Agent in K3s . Launch AWS/EC2 instance In this section you will use terraform to build an AWS/EC2 instance in your favorite AWS region and will automatically deploy the Kubernetes (K3s) environment that you will use in this Workshop. AWS Access Keys You will need access to an AWS account to obtain both AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY . Minimum requirements For the \u00b5APM module we are using the Hot R.O.D. application. The minimum requirements are: Hot R.O.D AWS/EC2 Instance min. requirements: t2.micro 1 vCPU, 8Gb Disk, 1Gb Memory Prepare Terraform The first step is to go into the sub-directory where the Terraform files are located and initialise Terraform and upgrade the AWS Terraform Provider. Shell Command cd ec2 terraform init -upgrade Output ~/workshop/ec2$ terraform init -upgrade Initializing the backend... Initializing provider plugins... - Checking for available provider plugins... - Downloading plugin for provider \"aws\" (hashicorp/aws) 2.60.0... The following providers do not have any version constraints in configuration, so the latest version was installed. To prevent automatic upgrades to new major versions that may contain breaking changes, it is recommended to add version = \"...\" constraints to the corresponding provider blocks in configuration, with the constraint strings suggested below. * provider.aws: version = \"~> 2.60\" Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. Create AWS/EC2 Instance Creating the AWS/EC2 instance is done in two steps, a planning phase and an apply phase. The planning phase will validate the Terraform scripts and check what changes it will make to your AWS environment. The apply phase will actually create the instance. First, you need to create environment variables for your AWS access keys. Shell Command export AWS_ACCESS_KEY_ID = \" YOUR_AWS_ACCESS_KEY_ID \" export AWS_SECRET_ACCESS_KEY = \" YOUR_AWS_SECRET_ACCESS_KEY \" echo $AWS_ACCESS_KEY_ID $AWS_SECRET_ACCESS_KEY Output ID: Axxxxxxxxxxxxxxxxy, KEY: Axxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb Once you have confirmed that you have set you AWS_SECRET_ACCESS_KEY_ID & AWS_SECRET_ACCESS_KEY correctly, you can start with the planning phase. Important Desired AWS Region : (Any AWS region by name, for example us-west-2 ) Please remember these values as you will need them again for the planning phase and when you use Terraform to destroy your AWS/EC2 instance. As we only wish to provide the input once, we are going to capture the output in a .out file that we can use for the apply step. Please provide your initials for the output file as indicated. Shell Command terraform plan -var = \"aws_instance_count=1\" -var = \"instance_type=1\" -out = observability-plan.out Enter your desired AWS Region where you wish to run the AWS/EC2 instance e.g. us-west-2 Example var.aws_region Provide the desired region Enter a value: us-west-2 Output Refreshing Terraform state in-memory prior to plan... The refreshed state will be used to calculate this plan, but will not be persisted to local or remote state storage. data.aws_ami.latest-ubuntu: Refreshing state... ------------------------------------------------------------------------ An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: **(BIG WALL OF AWS RELATED TEXT REMOVED)** Plan: 2 to add, 0 to change, 0 to destroy. ------------------------------------------------------------------------ This plan was saved to: observability-plan.out To perform exactly these actions, run the following command to apply: terraform apply \"observability-plan.out\" If there are no errors in the output and terraform has created your output file, you can start the apply phase of Terraform. This will create the AWS/EC2 instance. Shell Command terraform apply \"observability-plan.out\" Output ws_security_group.instance: Creating... aws_security_group.instance: Creation complete after 2s [id=sg-0459afecae5953b51] aws_instance.observability-instance[0]: Creating... aws_instance.observability-instance[0]: Still creating... [10s elapsed] aws_instance.observability-instance[0]: Still creating... [20s elapsed] aws_instance.observability-instance[0]: Creation complete after 23s [id=i-095a12cd39f8e2283] Apply complete! Resources: 2 added, 0 changed, 0 destroyed. The state of your infrastructure has been saved to the path below. This state is required to modify and destroy your infrastructure, so keep it safe. To inspect the complete state use the `terraform show` command. State path: terraform.tfstate Outputs: ip = [ \"YOUR IP ADDRESS\", ] Verify there are no errors and copy the ip address that you see in the green output. SSH into AWS/EC2 Instance Once the instance has been successfully created (this can take several minutes), ssh into it. In most cases your ssh client will ask you to verify the connection. Shell Command ssh ubuntu@YOUR IP ADDRESS Output The authenticity of host 'YOUR IP ADDRESS (YOUR IP ADDRESS)' can't be established. ECDSA key fingerprint is SHA256:XdqN55g0z/ER660PARM+mGqtpYpwM3333YS9Ac8Y9hLY. Are you sure you want to continue connecting (yes/no/[fingerprint])? Please confirm that you wish to continue by replying to the prompt with yes Shell Command Are you sure you want to continue connecting ( yes/no/ [ fingerprint ]) ? yes Output Warning: Permanently added 'YOUR IP ADDRESS' (ECDSA) to the list of known hosts. ubuntu@YOUR IP ADDRESS's password: To login to your instance please use the password provided by the Workshop host. Shell Command ubuntu@YOUR IP ADDRESS ' s password: PASSWORD Output \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u255d \u255a\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2554\u255d \u255a\u2588\u2588\u2557 \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2588\u2588\u2557 \u2588\u2588\u2554\u255d \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2557 \u2588\u2588\u2554\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d To run a command as administrator (user \"root\"), use \"sudo <command>\". See \"man sudo_root\" for details Waiting for cloud-init status... Your instance is ready! ubuntu@ip-172-31-41-196:~$ Once your instance presents you with the Splunk logo, make sure you see Your instance is ready! in the output. You have now completed the preparation for your AWS/EC2 instance and can go directly to the next lab Deploy the Smart Agent in K3s .","title":"3. Launch Instance"},{"location":"smartagent/prep/#4-about-signalfx-realms","text":"A realm is a self-contained deployment of SignalFx in which your organization is hosted. Different realms have different API endpoints (e.g. the endpoint for sending data is ingest.us1.signalfx.com for the us1 realm, and ingest.eu0.signalfx.com for the eu0 realm). Various instructions in this Workshop include a REALM placeholder that will need to be replaced with the actual name of your realm. This realm name is shown on your profile page in SignalFx. If you do not include the realm name when specifying an endpoint, SignalFx will interpret it as pointing to the us0 realm. Multipass is a lightweight VM manager for Linux, Windows and macOS. It's designed for developers who want a fresh Ubuntu environment with a single command. It uses KVM on Linux, Hyper-V on Windows and HyperKit on macOS to run the VM with minimal overhead. It can also use VirtualBox on Windows and macOS. Multipass will fetch images for you and keep them up to date. \u21a9 The SignalFx Smart Agent gathers host performance, application, and service-level metrics from both containerized and non-container environments. The Smart Agent installs with more than 100 bundled monitors for gathering data, including Python-based plug-ins such as Mongo, Redis, and Docker. \u21a9 What is Kubernetes? \u21a9 What is NGINX? \u21a9 What is Hot R.O.D.? \u21a9","title":"4. About SignalFx Realms"},{"location":"splunk/","text":"Step 1: Sign-up for a free trial account \u00b6 Sign up for a free trial account. Step 2: Access your trial account \u00b6 After you are logged into Splunk.com click on Free Splunk Then click Access Free 15-day trial Step 3: Login to your Splunk Cloud Environment \u00b6 Check your email for getting URL and credentials to your Splunk Cloud Environment. Make sure to check spam folder or search for 'Splunk Cloud' if you do not see the email. Login to your Splunk Cloud. You will be asked to change the password the first time you login.","title":"Signing Up"},{"location":"splunk/#step-1-sign-up-for-a-free-trial-account","text":"Sign up for a free trial account.","title":"Step 1: Sign-up for a free trial account"},{"location":"splunk/#step-2-access-your-trial-account","text":"After you are logged into Splunk.com click on Free Splunk Then click Access Free 15-day trial","title":"Step 2: Access your trial account"},{"location":"splunk/#step-3-login-to-your-splunk-cloud-environment","text":"Check your email for getting URL and credentials to your Splunk Cloud Environment. Make sure to check spam folder or search for 'Splunk Cloud' if you do not see the email. Login to your Splunk Cloud. You will be asked to change the password the first time you login.","title":"Step 3: Login to your Splunk Cloud Environment"},{"location":"splunk/datalinks/","text":"Integration with Splunk \u00b6 1. Introduction to Data links \u00b6 Data Links allow you to create dynamic links on properties that appear in a chart\u2019s data table and in list charts. Once enable this allows you to swiftly jump into external systems or SignalFx dashboards. 2. Configuring an integration with Splunk \u00b6 Goto Settings \u2192 Organizations Settings \u2192 Global Data Links and click on New Link Here you will need to provide a Link Label e.g. Examine in Splunk. For the Link to use the dropdown and select Custom URL . For Trigger there are 3 possible options: Any Metadata value - To display your link next to every property and every property value in a chart, select Any Metadata Value. Any Value of - To display your link next to a specified property type, select Any Value of. Property:Value Pair - To display your link next to a specified property value within a specified property type, select Property: Value Pair. To maximize the integration with Splunk, you are probably best selecting Any Metadata Value The Custom URL value will be the your Splunk instance. In this workshop, we will configure two Data Links - 1. for mapping pod specific data, and 2. for workload logs and events While configuring use Trigger > Any Value Of > kubernetes_pod_name as shown in the picture above. For Custom URL, you can copy the following and remember to replace the host of your Splunk Cloud https://prd-p-2i3b7.splunkcloud.com/en-US/app/search/search?q=index=\"em_events\" object.involvedObject.name={{properties.kubernetes_pod_name}} Similarly to set up workload data link select the properties as shown below For Custom URL, you can copy the following and remember to replace the host of your Splunk Cloud https://prd-p-2i3b7.splunkcloud.com/en-US/app/search/search?q=index=\"em_events\" \"{{properties.kubernetes_workload_name}}\" 3. Using Data Links \u00b6 Once the Data Link is configured any Data Table, List Chart or from Kubernetes Navigator will provide an ellipsis ( ... ) allowing you to Examine in Splunk . For linking pod logs and events For linking workload logs and events","title":"Setting up Data Links for Contextual Logging"},{"location":"splunk/datalinks/#integration-with-splunk","text":"","title":"Integration with Splunk"},{"location":"splunk/datalinks/#1-introduction-to-data-links","text":"Data Links allow you to create dynamic links on properties that appear in a chart\u2019s data table and in list charts. Once enable this allows you to swiftly jump into external systems or SignalFx dashboards.","title":"1. Introduction to Data links"},{"location":"splunk/datalinks/#2-configuring-an-integration-with-splunk","text":"Goto Settings \u2192 Organizations Settings \u2192 Global Data Links and click on New Link Here you will need to provide a Link Label e.g. Examine in Splunk. For the Link to use the dropdown and select Custom URL . For Trigger there are 3 possible options: Any Metadata value - To display your link next to every property and every property value in a chart, select Any Metadata Value. Any Value of - To display your link next to a specified property type, select Any Value of. Property:Value Pair - To display your link next to a specified property value within a specified property type, select Property: Value Pair. To maximize the integration with Splunk, you are probably best selecting Any Metadata Value The Custom URL value will be the your Splunk instance. In this workshop, we will configure two Data Links - 1. for mapping pod specific data, and 2. for workload logs and events While configuring use Trigger > Any Value Of > kubernetes_pod_name as shown in the picture above. For Custom URL, you can copy the following and remember to replace the host of your Splunk Cloud https://prd-p-2i3b7.splunkcloud.com/en-US/app/search/search?q=index=\"em_events\" object.involvedObject.name={{properties.kubernetes_pod_name}} Similarly to set up workload data link select the properties as shown below For Custom URL, you can copy the following and remember to replace the host of your Splunk Cloud https://prd-p-2i3b7.splunkcloud.com/en-US/app/search/search?q=index=\"em_events\" \"{{properties.kubernetes_workload_name}}\"","title":"2. Configuring an integration with Splunk"},{"location":"splunk/datalinks/#3-using-data-links","text":"Once the Data Link is configured any Data Table, List Chart or from Kubernetes Navigator will provide an ellipsis ( ... ) allowing you to Examine in Splunk . For linking pod logs and events For linking workload logs and events","title":"3. Using Data Links"},{"location":"splunk/hectoken/","text":"Create Splunk Http Event Collector (HEC) \u00b6 The HTTP Event Collector (HEC) is a fast and efficient way to send data over HTTP (or HTTPS) directly to Splunk Cloud from your application, Amazon EKS Kubernetes cluster in our case. Click Settings > Data Inputs in Splunk Cloud Click on \"Add New\" next to HTTP Event Collector Pick appropriate name such as k8s-hec, leave other fields as default Pick the clusters we created in the step \u2013 em_meta and em_events Review and Submit Copy Token Value","title":"Creating HEC Token"},{"location":"splunk/hectoken/#create-splunk-http-event-collector-hec","text":"The HTTP Event Collector (HEC) is a fast and efficient way to send data over HTTP (or HTTPS) directly to Splunk Cloud from your application, Amazon EKS Kubernetes cluster in our case. Click Settings > Data Inputs in Splunk Cloud Click on \"Add New\" next to HTTP Event Collector Pick appropriate name such as k8s-hec, leave other fields as default Pick the clusters we created in the step \u2013 em_meta and em_events Review and Submit Copy Token Value","title":"Create Splunk Http Event Collector (HEC)"},{"location":"splunk/sck/","text":"Setting up Kubernetes Logging by Deploying Splunk Connect for Kubernetes \u00b6 Shell Command cd splunk export HEC_TOKEN=# paste your hec token value export HEC_HOST=# paste Splunk Cloud host e.g. prd-p-ox1qg.splunkcloud.com Deploy SCK as a DaemonSet using Helm in Kubernetes namespace splunk Shell Command kubectl create ns splunk Deploy SCK Helm Chart Shell Command helm install \\ splunk --set global.splunk.hec.token=$HEC_TOKEN \\ --set global.splunk.hec.host=$HEC_HOST \\ --namespace splunk -f values.yaml https://github.com/splunk/splunk-connect-for-kubernetes/releases/download/1.4.1/splunk-connect-for-kubernetes-1.4.1.tgz OUTPUT NAME: splunk LAST DEPLOYED: Sat Jul 25 21:12:34 2020 NAMESPACE: splunk STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u255d\u255a\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2554\u255d \u255a\u2588\u2588\u2557 \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2588\u2588\u2557 \u2588\u2588\u2554\u255d \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2557\u2588\u2588\u2554\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u255d Listen to your data. Splunk Connect for Kubernetes is spinning up in your cluster. After a few minutes, you should see data being indexed in your Splunk. If you get stuck, we're here to help. Look for answers here: http://docs.splunk.com","title":"Deploying Splunk Connect for Kubernetes"},{"location":"splunk/sck/#setting-up-kubernetes-logging-by-deploying-splunk-connect-for-kubernetes","text":"Shell Command cd splunk export HEC_TOKEN=# paste your hec token value export HEC_HOST=# paste Splunk Cloud host e.g. prd-p-ox1qg.splunkcloud.com Deploy SCK as a DaemonSet using Helm in Kubernetes namespace splunk Shell Command kubectl create ns splunk Deploy SCK Helm Chart Shell Command helm install \\ splunk --set global.splunk.hec.token=$HEC_TOKEN \\ --set global.splunk.hec.host=$HEC_HOST \\ --namespace splunk -f values.yaml https://github.com/splunk/splunk-connect-for-kubernetes/releases/download/1.4.1/splunk-connect-for-kubernetes-1.4.1.tgz OUTPUT NAME: splunk LAST DEPLOYED: Sat Jul 25 21:12:34 2020 NAMESPACE: splunk STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u255d\u255a\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2554\u255d \u255a\u2588\u2588\u2557 \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2588\u2588\u2557 \u2588\u2588\u2554\u255d \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2557\u2588\u2588\u2554\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u255d Listen to your data. Splunk Connect for Kubernetes is spinning up in your cluster. After a few minutes, you should see data being indexed in your Splunk. If you get stuck, we're here to help. Look for answers here: http://docs.splunk.com","title":"Setting up Kubernetes Logging by Deploying Splunk Connect for Kubernetes"},{"location":"splunk/solution/","text":"Exercises and solutions \u00b6 Your mission, should you choose to accept it is to solve the mystery why ImageSample and Hungry-job are stuck in Pending status. Hint: Use monitoring and troubleshooting workflow from SignalFx to Splunk Cloud. Contextual logs will guide you towards the solutions. Fixes are available in /fixes directory. Hungry Cron Job is requesting too many resources. 5000 CPU millicores (5 whole CPUs) and 32GB of memory! That is why Kubernetes is not scheduling the pod even though it is being requested every minute by the cronjob. Let's fix these resource requests to a more reasonable number Shell Command kubectl patch cronjob hungry-job --patch=\"$(cat apps/fixes/hungry-fix.yaml)\" ImageSample pod is pending because it was trying to use an image which does not exist in the container registry provided in the pod spec. Shell Command kubectl patch deployment imagesample --patch=\"$(cat apps/fixes/imagesample-fix.yaml)\" After a few seconds check Kubernetes Navigator to make sure your patches have been applied successfully.","title":"Mission Possible"},{"location":"splunk/solution/#exercises-and-solutions","text":"Your mission, should you choose to accept it is to solve the mystery why ImageSample and Hungry-job are stuck in Pending status. Hint: Use monitoring and troubleshooting workflow from SignalFx to Splunk Cloud. Contextual logs will guide you towards the solutions. Fixes are available in /fixes directory. Hungry Cron Job is requesting too many resources. 5000 CPU millicores (5 whole CPUs) and 32GB of memory! That is why Kubernetes is not scheduling the pod even though it is being requested every minute by the cronjob. Let's fix these resource requests to a more reasonable number Shell Command kubectl patch cronjob hungry-job --patch=\"$(cat apps/fixes/hungry-fix.yaml)\" ImageSample pod is pending because it was trying to use an image which does not exist in the container registry provided in the pod spec. Shell Command kubectl patch deployment imagesample --patch=\"$(cat apps/fixes/imagesample-fix.yaml)\" After a few seconds check Kubernetes Navigator to make sure your patches have been applied successfully.","title":"Exercises and solutions"},{"location":"splunk/splunkindex/","text":"Creating Splunk Index \u00b6 The index is the repository for Splunk Enterprise data. Splunk Cloud transforms incoming data into events, which it stores in indexes. From the home page, Click on Settings and from expanded menu click Indexes Click New Index with Index Name em_meta . Leave Index Data Type selected as Events. Put 0 in Max raw data size and Searchable time (days) 15. Click Save to create the index. Following similar steps create another Index with name em_events","title":"Creating Index"},{"location":"splunk/splunkindex/#creating-splunk-index","text":"The index is the repository for Splunk Enterprise data. Splunk Cloud transforms incoming data into events, which it stores in indexes. From the home page, Click on Settings and from expanded menu click Indexes Click New Index with Index Name em_meta . Leave Index Data Type selected as Events. Put 0 in Max raw data size and Searchable time (days) 15. Click Save to create the index. Following similar steps create another Index with name em_events","title":"Creating Splunk Index"}]}